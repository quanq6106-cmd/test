{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":3424966,"sourceType":"datasetVersion","datasetId":2035671},{"sourceId":3818163,"sourceType":"datasetVersion","datasetId":2274483},{"sourceId":4501486,"sourceType":"datasetVersion","datasetId":2631784},{"sourceId":5258124,"sourceType":"datasetVersion","datasetId":3059801},{"sourceId":9971715,"sourceType":"datasetVersion","datasetId":6134857},{"sourceId":11087772,"sourceType":"datasetVersion","datasetId":4576291},{"sourceId":11497644,"sourceType":"datasetVersion","datasetId":7207743},{"sourceId":11564740,"sourceType":"datasetVersion","datasetId":7251054},{"sourceId":11739593,"sourceType":"datasetVersion","datasetId":5773627},{"sourceId":363168,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":301540,"modelId":322000}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELL 2 (REPLACE) — install xong KHÔNG import transformers ở đây (để tránh cache về disk trước)\n!pip -q install -U --no-cache-dir \\\n  \"transformers>=4.51.0\" \\\n  \"accelerate>=0.30.0\" \\\n  \"datasets>=2.19.0\" \\\n  \"peft>=0.11.0\" \\\n  \"trl>=0.9.6\" \\\n  \"bitsandbytes>=0.43.1\" \\\n  \"huggingface_hub>=0.23.0\" \\\n  \"tokenizers>=0.21.0\" \\\n  \"safetensors>=0.4.3\" \\\n  \"sentencepiece\" \\\n  \"jsonschema>=4.22.0\" \\\n  \"rapidfuzz>=3.9.0\" \\\n  \"openpyxl>=3.1.5\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:41:15.094174Z","iopub.execute_input":"2026-01-22T06:41:15.094669Z","iopub.status.idle":"2026-01-22T06:41:19.972068Z","shell.execute_reply.started":"2026-01-22T06:41:15.094650Z","shell.execute_reply":"2026-01-22T06:41:19.971460Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# CELL 1 (REPLACE) — RAM mode phải chạy TRƯỚC MỌI import transformers/tokenizers/peft\nimport os\nfrom pathlib import Path\n\n!df -h /dev/shm\n\nRAM_BASE = Path(\"/dev/shm/kaggle_ram\")\nRAM_BASE.mkdir(parents=True, exist_ok=True)\n\nHF_HOME = RAM_BASE / \"hf\"\nHF_HOME.mkdir(parents=True, exist_ok=True)\n\nos.environ[\"HF_HOME\"] = str(HF_HOME)\nos.environ[\"HF_HUB_CACHE\"] = str(HF_HOME / \"hub\")\nos.environ[\"HF_DATASETS_CACHE\"] = str(HF_HOME / \"datasets\")\nos.environ[\"TRANSFORMERS_CACHE\"] = str(HF_HOME / \"transformers\")\nos.environ[\"TORCH_HOME\"] = str(RAM_BASE / \"torch\")\nos.environ[\"XDG_CACHE_HOME\"] = str(RAM_BASE / \".cache\")\n\n# outputs/logs cũng đẩy vào RAM\nWORKDIR = RAM_BASE / \"working\"\nDATA_DIR = WORKDIR / \"data\"\nTEACH_CACHE_DIR = WORKDIR / \"teacher_outputs\"\nWORKDIR.mkdir(parents=True, exist_ok=True)\nDATA_DIR.mkdir(parents=True, exist_ok=True)\nTEACH_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"HF_HOME =\", os.environ[\"HF_HOME\"])\nprint(\"HF_HUB_CACHE =\", os.environ[\"HF_HUB_CACHE\"])\nprint(\"WORKDIR =\", WORKDIR)\nprint(\"DATA_DIR =\", DATA_DIR)\nprint(\"TEACH_CACHE_DIR =\", TEACH_CACHE_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:41:19.973349Z","iopub.execute_input":"2026-01-22T06:41:19.973518Z","iopub.status.idle":"2026-01-22T06:41:20.089842Z","shell.execute_reply.started":"2026-01-22T06:41:19.973499Z","shell.execute_reply":"2026-01-22T06:41:20.089324Z"}},"outputs":[{"name":"stdout","text":"Filesystem      Size  Used Avail Use% Mounted on\nshm             114G     0  114G   0% /dev/shm\nHF_HOME = /dev/shm/kaggle_ram/hf\nHF_HUB_CACHE = /dev/shm/kaggle_ram/hf/hub\nWORKDIR = /dev/shm/kaggle_ram/working\nDATA_DIR = /dev/shm/kaggle_ram/working/data\nTEACH_CACHE_DIR = /dev/shm/kaggle_ram/working/teacher_outputs\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# CELL 3 (REPLACE) — setup PATHS/QWEN32B_PATH nhưng KHÔNG reset WORKDIR/DATA_DIR về /kaggle/working nữa\nimport os, re, json, time, math, random\nfrom glob import glob\n\n# WORKDIR/DATA_DIR đã được set từ CELL 1 (RAM). Không ghi đè lại.\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\ndef find_qwen32b_path():\n    candidates = []\n    for p in glob(\"/kaggle/input/**\", recursive=True):\n        if os.path.isdir(p):\n            low = p.lower()\n            if \"qwen\" in low and (\"32b\" in low or \"32-b\" in low):\n                if os.path.exists(os.path.join(p, \"config.json\")):\n                    candidates.append(p)\n    candidates = sorted(candidates, key=lambda x: len(x))\n    return candidates[0] if candidates else None\n\nQWEN32B_PATH = find_qwen32b_path()\nprint(\"QWEN32B_PATH =\", QWEN32B_PATH)\n\nTEACHERS = {\n    \"open_finance_8b\": \"DragonLLM/Llama-Open-Finance-8B\",\n    \"finance_llama3_8b\": \"instruction-pretrain/finance-Llama3-8B\",\n    \"fingpt_lora_llama3_8b\": \"FinGPT/fingpt-mt_llama3-8b_lora\",\n}\nFINGPT_BASE = \"meta-llama/Meta-Llama-3-8B\"\n\nPATHS = {\n    \"vn_mcocr\": \"/kaggle/input/vietnamese-receipts-mc-ocr-2021\",\n    \"invoice_ocr\": \"/kaggle/input/invoice-ocr\",\n    \"hi_quality_invoice\": \"/kaggle/input/high-quality-invoice-images-for-ocr\",\n    \"gl_xlsx\": \"/kaggle/input/generalledger/Data file for students.xlsx\",\n    \"transactions_csv\": \"/kaggle/input/financial-transactions-dataset/financial_transactions.csv\",\n    \"forecast_csv\": \"/kaggle/input/financial-forecasting-data/simulated_financial_forecasting_data.csv\",\n    \"data_retriever_csv\": \"/kaggle/input/data-retreiver/Data_ret.csv\",\n}\nprint(\"DATA PATHS OK\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:41:20.090651Z","iopub.execute_input":"2026-01-22T06:41:20.090826Z","iopub.status.idle":"2026-01-22T06:44:46.239136Z","shell.execute_reply.started":"2026-01-22T06:41:20.090809Z","shell.execute_reply":"2026-01-22T06:44:46.238377Z"}},"outputs":[{"name":"stdout","text":"QWEN32B_PATH = /kaggle/input/qwen-3/transformers/32b/1\nDATA PATHS OK\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from jsonschema import validate\nfrom jsonschema.exceptions import ValidationError\n\n# ===== Schemas =====\nRECEIPT_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"vendor_name\": {\"type\": [\"string\", \"null\"]},\n        \"address\": {\"type\": [\"string\", \"null\"]},\n        \"date\": {\"type\": [\"string\", \"null\"]},            # YYYY-MM-DD preferred\n        \"total_amount\": {\"type\": [\"number\", \"null\"]},\n        \"currency\": {\"type\": [\"string\", \"null\"]},        # \"VND\"\n        \"confidence\": {\"type\": \"number\"},\n        \"flags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n    },\n    \"required\": [\"vendor_name\",\"address\",\"date\",\"total_amount\",\"currency\",\"confidence\",\"flags\"]\n}\n\nINVOICE_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"vendor_name\": {\"type\": [\"string\", \"null\"]},\n        \"invoice_no\": {\"type\": [\"string\", \"null\"]},\n        \"date\": {\"type\": [\"string\", \"null\"]},\n        \"subtotal\": {\"type\": [\"number\", \"null\"]},\n        \"tax\": {\"type\": [\"number\", \"null\"]},\n        \"total\": {\"type\": [\"number\", \"null\"]},\n        \"currency\": {\"type\": [\"string\", \"null\"]},\n        \"confidence\": {\"type\": \"number\"},\n        \"flags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n    },\n    \"required\": [\"vendor_name\",\"invoice_no\",\"date\",\"subtotal\",\"tax\",\"total\",\"currency\",\"confidence\",\"flags\"]\n}\n\nJOURNAL_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"entries\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"account\": {\"type\": \"string\"},\n                    \"debit\": {\"type\": \"number\"},\n                    \"credit\": {\"type\": \"number\"},\n                    \"memo\": {\"type\": [\"string\",\"null\"]}\n                },\n                \"required\": [\"account\",\"debit\",\"credit\",\"memo\"]\n            }\n        },\n        \"confidence\": {\"type\": \"number\"},\n        \"flags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n    },\n    \"required\": [\"entries\",\"confidence\",\"flags\"]\n}\n\nTASK2SCHEMA = {\n    \"receipt_extract_text\": RECEIPT_SCHEMA,\n    \"invoice_extract_text\": INVOICE_SCHEMA,\n    \"journal_from_structured_txn\": JOURNAL_SCHEMA,\n}\n\ndef schema_pass(task: str, obj: dict) -> bool:\n    try:\n        validate(instance=obj, schema=TASK2SCHEMA[task])\n        return True\n    except ValidationError:\n        return False\n    except Exception:\n        return False\n\n# ===== JSON extract / repair =====\ndef extract_json_from_text(text: str):\n    if text is None:\n        return None\n    # pick first {...} block\n    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n    if not m:\n        return None\n    s = m.group(0)\n    try:\n        return json.loads(s)\n    except Exception:\n        return None\n\ndef json_repair_minimal(text: str):\n    \"\"\"\n    deterministic repair for common LLM issues:\n    - trailing commas\n    - single quotes -> double quotes (simple cases)\n    \"\"\"\n    if text is None:\n        return None\n    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n    if not m:\n        return None\n    s = m.group(0).strip()\n\n    s = re.sub(r\",\\s*}\", \"}\", s)\n    s = re.sub(r\",\\s*]\", \"]\", s)\n    # naive quote fix (only if it looks like JSON)\n    if \"'\" in s and '\"' not in s:\n        s = s.replace(\"'\", '\"')\n\n    try:\n        return json.loads(s)\n    except Exception:\n        return None\n\n# ===== Prompt builder =====\ndef build_prompt(task: str, input_data):\n    if task == \"receipt_extract_text\":\n        return f\"\"\"\nExtract receipt key fields from Vietnamese text.\nReturn ONLY valid JSON with fields:\nvendor_name,address,date,total_amount,currency,confidence,flags\n\nReceipt Text:\n{input_data}\n\"\"\".strip()\n\n    if task == \"invoice_extract_text\":\n        return f\"\"\"\nExtract invoice fields from text.\nReturn ONLY valid JSON with fields:\nvendor_name,invoice_no,date,subtotal,tax,total,currency,confidence,flags\n\nInvoice Text:\n{input_data}\n\"\"\".strip()\n\n    if task == \"journal_from_structured_txn\":\n        return f\"\"\"\nYou are an ERP accountant.\nGiven a structured transaction JSON, propose journal entries.\nReturn ONLY valid JSON with fields:\n- entries: array of objects (account, debit, credit, memo)\n- confidence\n- flags\n\nTransaction:\n{json.dumps(input_data, ensure_ascii=False)}\n\"\"\".strip()\n\n    raise ValueError(\"Unknown task\")\n\nprint(\"Schemas + prompt builder ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:44:46.239826Z","iopub.execute_input":"2026-01-22T06:44:46.239977Z","iopub.status.idle":"2026-01-22T06:44:47.221820Z","shell.execute_reply.started":"2026-01-22T06:44:46.239962Z","shell.execute_reply":"2026-01-22T06:44:47.221347Z"}},"outputs":[{"name":"stdout","text":"Schemas + prompt builder ready.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\n\ndef infer_col(df, candidates):\n    cols = {c.lower(): c for c in df.columns}\n    for cand in candidates:\n        if cand.lower() in cols:\n            return cols[cand.lower()]\n    # fuzzy contains\n    for c in df.columns:\n        low = c.lower()\n        for cand in candidates:\n            if cand.lower() in low:\n                return c\n    return None\n\ndef load_vn_mcocr_cases(limit=300):\n    root = PATHS[\"vn_mcocr\"]\n    cases = []\n\n    # Prefer CSV with gold labels if present\n    csv_candidates = [\n        os.path.join(root, \"mcocr_train_df.csv\"),\n        os.path.join(root, \"mcocr_val_sample_df.csv\"),\n        os.path.join(root, \"results.csv\"),\n    ]\n    for p in csv_candidates:\n        if os.path.exists(p):\n            df = pd.read_csv(p)\n\n            text_col = infer_col(df, [\"text\", \"ocr_text\", \"raw_text\", \"content\", \"transcription\"])\n            if text_col is None:\n                # fallback pick longest string col\n                str_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n                if str_cols:\n                    text_col = max(str_cols, key=lambda c: df[c].astype(str).str.len().mean())\n\n            seller_col = infer_col(df, [\"seller\", \"vendor\", \"vendor_name\", \"merchant\", \"store\", \"shop\"])\n            addr_col   = infer_col(df, [\"address\", \"seller_address\", \"vendor_address\"])\n            date_col   = infer_col(df, [\"timestamp\", \"date\", \"datetime\", \"time\"])\n            total_col  = infer_col(df, [\"total_cost\", \"total\", \"amount\", \"total_amount\", \"sum\"])\n\n            for i, row in df.head(limit).iterrows():\n                raw_text = str(row[text_col]) if text_col else \"\"\n\n                gold = None\n                if seller_col or addr_col or date_col or total_col:\n                    def safe_float(x):\n                        try:\n                            if pd.isna(x): \n                                return None\n                            s = str(x)\n                            s = re.sub(r\"[^\\d\\.\\-]\", \"\", s)\n                            return float(s) if s else None\n                        except:\n                            return None\n\n                    gold = {\n                        \"vendor_name\": str(row[seller_col]) if seller_col and pd.notna(row[seller_col]) else None,\n                        \"address\": str(row[addr_col]) if addr_col and pd.notna(row[addr_col]) else None,\n                        \"date\": str(row[date_col]) if date_col and pd.notna(row[date_col]) else None,\n                        \"total_amount\": safe_float(row[total_col]) if total_col else None,\n                        \"currency\": \"VND\",\n                        \"confidence\": 0.0,\n                        \"flags\": []\n                    }\n\n                cases.append({\n                    \"id\": f\"vn_mcocr_{i}\",\n                    \"task\": \"receipt_extract_text\",\n                    \"input\": raw_text,\n                    \"gold\": gold,\n                    \"meta\": {\"source\": os.path.basename(p)}\n                })\n            return cases\n\n    # fallback txt (OCR lines)\n    txt_candidates = [\n        os.path.join(root, \"text_recognition_train_data.txt\"),\n        os.path.join(root, \"text_recognition_val_data.txt\"),\n    ]\n    for p in txt_candidates:\n        if os.path.exists(p):\n            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                for idx, line in enumerate(f):\n                    if idx >= limit:\n                        break\n                    parts = line.strip().split(\"\\t\")\n                    raw_text = parts[-1] if parts else \"\"\n                    cases.append({\n                        \"id\": f\"vn_mcocr_txt_{idx}\",\n                        \"task\": \"receipt_extract_text\",\n                        \"input\": raw_text,\n                        \"gold\": None,\n                        \"meta\": {\"source\": os.path.basename(p)}\n                    })\n            return cases\n\n    return []\n\ndef load_gl_cases(limit=200):\n    xlsx_path = PATHS[\"gl_xlsx\"]\n    if not os.path.exists(xlsx_path):\n        return []\n    xls = pd.ExcelFile(xlsx_path)\n    # take first sheet by default\n    df = pd.read_excel(xlsx_path, sheet_name=xls.sheet_names[0])\n\n    cases = []\n    for i, row in df.head(limit).iterrows():\n        txn = row.to_dict()\n        cases.append({\n            \"id\": f\"gl_{i}\",\n            \"task\": \"journal_from_structured_txn\",\n            \"input\": txn,\n            \"gold\": None,\n            \"meta\": {\"sheet\": xls.sheet_names[0]}\n        })\n    return cases\n\ndef load_invoice_ocr_cases(limit=200):\n    \"\"\"\n    Robust loader:\n    - If JSON/CSV annotations exist -> use their text fields\n    - Otherwise use image paths (text-only LLM can't read images, but still valid for KD if you later OCR)\n    \"\"\"\n    root = PATHS[\"invoice_ocr\"]\n    if not os.path.exists(root):\n        return []\n\n    ann_files = []\n    for ext in [\"*.json\",\"*.csv\"]:\n        ann_files += glob(os.path.join(root, \"**\", ext), recursive=True)\n\n    cases = []\n    if ann_files:\n        # take first annotation file found\n        p = ann_files[0]\n        if p.endswith(\".csv\"):\n            df = pd.read_csv(p)\n            text_col = infer_col(df, [\"text\",\"ocr\",\"raw\",\"content\"])\n            for i, row in df.head(limit).iterrows():\n                raw_text = str(row[text_col]) if text_col else \"\"\n                cases.append({\n                    \"id\": f\"invoice_ocr_csv_{i}\",\n                    \"task\": \"invoice_extract_text\",\n                    \"input\": raw_text,\n                    \"gold\": None,\n                    \"meta\": {\"ann\": os.path.basename(p)}\n                })\n        else:\n            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                js = json.load(f)\n            # try to find list items with \"text\"\n            items = []\n            if isinstance(js, list):\n                items = js\n            elif isinstance(js, dict):\n                # common keys\n                for k in [\"data\",\"items\",\"annotations\",\"samples\"]:\n                    if k in js and isinstance(js[k], list):\n                        items = js[k]\n                        break\n\n            for i, it in enumerate(items[:limit]):\n                raw_text = it.get(\"text\") or it.get(\"ocr_text\") or it.get(\"content\") or \"\"\n                cases.append({\n                    \"id\": f\"invoice_ocr_json_{i}\",\n                    \"task\": \"invoice_extract_text\",\n                    \"input\": str(raw_text),\n                    \"gold\": None,\n                    \"meta\": {\"ann\": os.path.basename(p)}\n                })\n\n        return cases\n\n    # fallback: use image paths (for later OCR pipeline)\n    imgs = glob(os.path.join(root, \"**\", \"*.png\"), recursive=True) + glob(os.path.join(root, \"**\", \"*.jpg\"), recursive=True)\n    for i, ip in enumerate(imgs[:limit]):\n        cases.append({\n            \"id\": f\"invoice_ocr_img_{i}\",\n            \"task\": \"invoice_extract_text\",\n            \"input\": f\"[IMAGE_PATH]{ip}\",\n            \"gold\": None,\n            \"meta\": {\"img\": os.path.basename(ip)}\n        })\n    return cases\n\nprint(\"Loaders ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:44:47.222991Z","iopub.execute_input":"2026-01-22T06:44:47.223166Z","iopub.status.idle":"2026-01-22T06:44:47.556664Z","shell.execute_reply.started":"2026-01-22T06:44:47.223150Z","shell.execute_reply":"2026-01-22T06:44:47.556194Z"}},"outputs":[{"name":"stdout","text":"Loaders ready.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# CELL SPLIT (REPLACE) — tách MCOCR (gold) riêng + tạo kd_pool lớn hơn nhưng có kiểm soát\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, date\nimport random, json, re\nfrom pathlib import Path\n\ndef _json_default(o):\n    if isinstance(o, (pd.Timestamp, datetime, date)):\n        return o.isoformat()\n    if isinstance(o, (np.integer,)):\n        return int(o)\n    if isinstance(o, (np.floating,)):\n        return float(o)\n    if isinstance(o, (np.ndarray,)):\n        return o.tolist()\n    return str(o)\n\ndef write_jsonl(path, rows):\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        for r in rows:\n            f.write(json.dumps(r, ensure_ascii=False, default=_json_default) + \"\\n\")\n\n# 1) Load dữ liệu\nmcocr_cases = load_vn_mcocr_cases(limit=2000)          # tăng để có nhiều gold\ninvoice_cases = load_invoice_ocr_cases(limit=2000)     # tăng, nhưng thường thiếu gold\ngl_cases = load_gl_cases(limit=2000)\n\n# 2) Split MCOCR có gold để EVAL chuẩn\nmcocr_gold = [c for c in mcocr_cases if isinstance(c.get(\"gold\"), dict)]\nrandom.shuffle(mcocr_gold)\n\n# Giữ eval_gold đủ lớn để đo; train_gold còn lại để KD/augment nếu muốn\nN_EVAL_GOLD = min(500, len(mcocr_gold))\neval_gold_cases = mcocr_gold[:N_EVAL_GOLD]\ntrain_gold_cases = mcocr_gold[N_EVAL_GOLD:]\n\n# 3) Eval set: ưu tiên MCOCR gold + thêm journal để đo schema/journal\neval_cases = []\neval_cases += eval_gold_cases\n# thêm journal (không có gold) để đo schema strict\neval_cases += gl_cases[:500]\n\n# (tuỳ chọn) invoice text-only nếu có (nếu input là IMAGE_PATH thì eval text-only LLM sẽ fail => bỏ)\ninvoice_text_only = [c for c in invoice_cases if isinstance(c.get(\"input\"), str) and not str(c[\"input\"]).startswith(\"[IMAGE_PATH]\")]\neval_cases += invoice_text_only[:300]\n\nrandom.shuffle(eval_cases)\n\n# 4) KD pool: lấy (train_gold + invoice_text_only + gl) => lớn, đa dạng\nkd_pool = []\nkd_pool += train_gold_cases\nkd_pool += invoice_text_only\nkd_pool += gl_cases\nrandom.shuffle(kd_pool)\n\n# giới hạn để iteration ổn; bạn có thể nâng dần (2k, 5k, 10k)\nKD_POOL_MAX = min(5000, len(kd_pool))\nkd_pool = kd_pool[:KD_POOL_MAX]\n\nprint(\"MCOCR total:\", len(mcocr_cases), \"| gold:\", len(mcocr_gold))\nprint(\"EVAL cases:\", len(eval_cases), \"| EVAL gold:\", len(eval_gold_cases))\nprint(\"KD pool:\", len(kd_pool))\n\neval_path = str(DATA_DIR / \"eval_cases.jsonl\")\nkd_pool_path = str(DATA_DIR / \"kd_pool.jsonl\")\nwrite_jsonl(eval_path, eval_cases)\nwrite_jsonl(kd_pool_path, kd_pool)\n\nprint(\"Saved eval_path:\", eval_path)\nprint(\"Saved kd_pool_path:\", kd_pool_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:44:47.557321Z","iopub.execute_input":"2026-01-22T06:44:47.557579Z","iopub.status.idle":"2026-01-22T06:44:51.726187Z","shell.execute_reply.started":"2026-01-22T06:44:47.557560Z","shell.execute_reply":"2026-01-22T06:44:51.725726Z"}},"outputs":[{"name":"stdout","text":"MCOCR total: 1155 | gold: 0\nEVAL cases: 500 | EVAL gold: 0\nKD pool: 2000\nSaved eval_path: /dev/shm/kaggle_ram/working/data/eval_cases.jsonl\nSaved kd_pool_path: /dev/shm/kaggle_ram/working/data/kd_pool.jsonl\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# =======================\n# CELL 4B (REPLACE)\n# =======================\nimport os\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\nHF_CACHE_DIR = os.environ[\"HF_HUB_CACHE\"]\n\ndef assert_cuda_bf16_ready():\n    assert torch.cuda.is_available(), \"CUDA is not available. You said H100 runtime.\"\n    name = torch.cuda.get_device_name(0)\n    print(\"CUDA device:\", name)\n    # H100 supports bf16\n    assert torch.cuda.is_bf16_supported(), \"bf16 not supported on this GPU runtime.\"\n    return name\n\ndef load_bf16_model(repo_or_path: str):\n    \"\"\"\n    Full BF16 weights on GPU. Use this for BOTH training+eval if you want pure BF16.\n    \"\"\"\n    assert_cuda_bf16_ready()\n\n    tok = AutoTokenizer.from_pretrained(\n        repo_or_path,\n        use_fast=True,\n        trust_remote_code=True,\n        cache_dir=HF_CACHE_DIR,\n    )\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n    tok.padding_side = \"left\"\n\n    mdl = AutoModelForCausalLM.from_pretrained(\n        repo_or_path,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16,\n        trust_remote_code=True,\n        attn_implementation=\"flash_attention_2\",\n        cache_dir=HF_CACHE_DIR,\n    )\n    mdl.eval()\n    return tok, mdl\n\n@torch.no_grad()\ndef generate_batch(tok, mdl, prompts, max_new_tokens=256):\n    enc = tok(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(mdl.device)\n    out = mdl.generate(\n        **enc,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        top_p=1.0,\n        repetition_penalty=1.05,\n        pad_token_id=tok.eos_token_id,\n    )\n    in_len = enc[\"input_ids\"].shape[1]\n    gen = out[:, in_len:]\n    return tok.batch_decode(gen, skip_special_tokens=True)\n\nprint(\"Model utils ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:44:51.726899Z","iopub.execute_input":"2026-01-22T06:44:51.727145Z","iopub.status.idle":"2026-01-22T06:45:07.442415Z","shell.execute_reply.started":"2026-01-22T06:44:51.727128Z","shell.execute_reply":"2026-01-22T06:45:07.441946Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model utils ready.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# CELL 5 (REPLACE) — teacher cell: không hardcode token + TEACH_CACHE_DIR dùng RAM\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nsecret_value_1 = user_secrets.get_secret(\"HF_TOKEN\")\n\nHF_TOKEN = os.getenv(\"HF_TOKEN\", secret_value_1)\nif HF_TOKEN:\n    login(token=HF_TOKEN)\n    print(\"HF login OK\")\nelse:\n    print(\"HF_TOKEN missing -> gated teachers may fail\")\n\nTEACH_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n\ndef run_teacher(name, repo=None, adapter=None, base=None, batch_size=16):\n    print(f\"\\n=== Teacher: {name} ===\")\n    try:\n        if adapter and base:\n            tok, mdl = load_fingpt(adapter, base)\n        else:\n            tok, mdl = load_4bit_model(repo)\n\n        with open(kd_pool_path, \"r\", encoding=\"utf-8\") as f:\n            pool = [json.loads(x) for x in f]\n\n        out_path = TEACH_CACHE_DIR / f\"{name}.jsonl\"\n        with open(out_path, \"w\", encoding=\"utf-8\") as fw:\n            for i in range(0, len(pool), batch_size):\n                batch = pool[i:i+batch_size]\n                prompts = [build_prompt(ex[\"task\"], ex[\"input\"]) for ex in batch]\n                texts = generate_batch(tok, mdl, prompts)\n\n                for ex, t in zip(batch, texts):\n                    fw.write(json.dumps({\"id\": ex[\"id\"], \"task\": ex[\"task\"], \"raw\": t}, ensure_ascii=False) + \"\\n\")\n\n        del mdl\n        torch.cuda.empty_cache()\n        print(\"Saved:\", str(out_path))\n        return str(out_path)\n    except Exception as e:\n        print(\"FAILED:\", name, \"| reason:\", str(e))\n        return None\n\n\nteacher_paths = {}\n\n# finance llama3 (public)\nteacher_paths[\"finance_llama3_8b\"] = run_teacher(\"finance_llama3_8b\", repo=TEACHERS[\"finance_llama3_8b\"])\n\n# open finance (gated)\nteacher_paths[\"open_finance_8b\"] = run_teacher(\"open_finance_8b\", repo=TEACHERS[\"open_finance_8b\"])\n\n# fingpt lora (needs base llama3 gated)\nteacher_paths[\"fingpt_lora_llama3_8b\"] = run_teacher(\n    \"fingpt_lora_llama3_8b\",\n    adapter=TEACHERS[\"fingpt_lora_llama3_8b\"],\n    base=FINGPT_BASE\n)\n\nprint(\"\\nTeacher paths:\", teacher_paths)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:45:07.443103Z","iopub.execute_input":"2026-01-22T06:45:07.443419Z","iopub.status.idle":"2026-01-22T06:45:07.741514Z","shell.execute_reply.started":"2026-01-22T06:45:07.443402Z","shell.execute_reply":"2026-01-22T06:45:07.741057Z"}},"outputs":[{"name":"stdout","text":"HF login OK\n\n=== Teacher: finance_llama3_8b ===\nFAILED: finance_llama3_8b | reason: name 'load_4bit_model' is not defined\n\n=== Teacher: open_finance_8b ===\nFAILED: open_finance_8b | reason: name 'load_4bit_model' is not defined\n\n=== Teacher: fingpt_lora_llama3_8b ===\nFAILED: fingpt_lora_llama3_8b | reason: name 'load_fingpt' is not defined\n\nTeacher paths: {'finance_llama3_8b': None, 'open_finance_8b': None, 'fingpt_lora_llama3_8b': None}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# CELL DISTILL (REPLACE) — robust JSON extraction + coercion so KD distilled != 0\nimport re, json\nfrom datetime import datetime\n\ndef _strip_code_fences(s: str) -> str:\n    if not s:\n        return s\n    s = s.strip()\n    # remove ```json ... ``` or ``` ... ```\n    s = re.sub(r\"^```(?:json)?\\s*\", \"\", s, flags=re.I)\n    s = re.sub(r\"\\s*```$\", \"\", s)\n    return s.strip()\n\ndef _try_json_load(s: str):\n    try:\n        return json.loads(s)\n    except Exception:\n        return None\n\ndef _json_repair_minimal(s: str):\n    if s is None:\n        return None\n    s = _strip_code_fences(s)\n    s = s.strip()\n\n    # remove trailing commas\n    s = re.sub(r\",\\s*}\", \"}\", s)\n    s = re.sub(r\",\\s*]\", \"]\", s)\n\n    # if single quotes and no double quotes (naive)\n    if \"'\" in s and '\"' not in s:\n        s = s.replace(\"'\", '\"')\n\n    return _try_json_load(s)\n\ndef extract_json_robust(text: str):\n    \"\"\"\n    Strategy:\n    1) try whole text (after stripping fences)\n    2) try JSON code block\n    3) scan all {...} candidates and pick the first that parses\n    \"\"\"\n    if text is None:\n        return None\n\n    t = _strip_code_fences(text)\n\n    # 1) whole\n    obj = _try_json_load(t)\n    if obj is not None:\n        return obj\n\n    # 2) try inside ```json ... ```\n    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", text, flags=re.S | re.I)\n    if m:\n        obj = _try_json_load(m.group(1))\n        if obj is not None:\n            return obj\n        obj = _json_repair_minimal(m.group(1))\n        if obj is not None:\n            return obj\n\n    # 3) scan brace blocks non-greedy\n    for m in re.finditer(r\"\\{.*?\\}\", t, flags=re.S):\n        cand = m.group(0)\n        obj = _try_json_load(cand)\n        if obj is not None:\n            return obj\n        obj = _json_repair_minimal(cand)\n        if obj is not None:\n            return obj\n\n    return None\n\ndef _to_float(x):\n    if x is None:\n        return None\n    if isinstance(x, (int, float)):\n        return float(x)\n    s = str(x)\n    s = s.replace(\",\", \".\")\n    s = re.sub(r\"[^0-9\\.\\-]\", \"\", s)\n    try:\n        return float(s) if s else None\n    except Exception:\n        return None\n\ndef _to_str_or_none(x):\n    if x is None:\n        return None\n    s = str(x).strip()\n    return s if s else None\n\ndef _to_flags(x):\n    if x is None:\n        return []\n    if isinstance(x, list):\n        return [str(v) for v in x if str(v).strip()]\n    # split by comma/newline\n    s = str(x)\n    parts = re.split(r\"[,;\\n]+\", s)\n    return [p.strip() for p in parts if p.strip()]\n\ndef coerce_to_schema(task: str, obj: dict):\n    if not isinstance(obj, dict):\n        return None\n\n    if task == \"receipt_extract_text\":\n        out = {\n            \"vendor_name\": _to_str_or_none(obj.get(\"vendor_name\") or obj.get(\"vendor\") or obj.get(\"seller\")),\n            \"address\": _to_str_or_none(obj.get(\"address\") or obj.get(\"addr\")),\n            \"date\": _to_str_or_none(obj.get(\"date\")),\n            \"total_amount\": _to_float(obj.get(\"total_amount\") or obj.get(\"total\") or obj.get(\"amount\")),\n            \"currency\": _to_str_or_none(obj.get(\"currency\")) or \"VND\",\n            \"confidence\": _to_float(obj.get(\"confidence\")) if obj.get(\"confidence\") is not None else 0.5,\n            \"flags\": _to_flags(obj.get(\"flags\")),\n        }\n        return out\n\n    if task == \"invoice_extract_text\":\n        out = {\n            \"vendor_name\": _to_str_or_none(obj.get(\"vendor_name\") or obj.get(\"vendor\") or obj.get(\"seller\")),\n            \"invoice_no\": _to_str_or_none(obj.get(\"invoice_no\") or obj.get(\"invoice_number\") or obj.get(\"invoice\")),\n            \"date\": _to_str_or_none(obj.get(\"date\")),\n            \"subtotal\": _to_float(obj.get(\"subtotal\")),\n            \"tax\": _to_float(obj.get(\"tax\") or obj.get(\"vat\")),\n            \"total\": _to_float(obj.get(\"total\") or obj.get(\"total_amount\") or obj.get(\"amount\")),\n            \"currency\": _to_str_or_none(obj.get(\"currency\")) or \"VND\",\n            \"confidence\": _to_float(obj.get(\"confidence\")) if obj.get(\"confidence\") is not None else 0.5,\n            \"flags\": _to_flags(obj.get(\"flags\")),\n        }\n        return out\n\n    if task == \"journal_from_structured_txn\":\n        entries = obj.get(\"entries\")\n        if not isinstance(entries, list):\n            entries = []\n        norm_entries = []\n        for e in entries:\n            if not isinstance(e, dict):\n                continue\n            norm_entries.append({\n                \"account\": _to_str_or_none(e.get(\"account\")) or \"\",\n                \"debit\": _to_float(e.get(\"debit\")) or 0.0,\n                \"credit\": _to_float(e.get(\"credit\")) or 0.0,\n                \"memo\": _to_str_or_none(e.get(\"memo\")),\n            })\n        out = {\n            \"entries\": norm_entries,\n            \"confidence\": _to_float(obj.get(\"confidence\")) if obj.get(\"confidence\") is not None else 0.5,\n            \"flags\": _to_flags(obj.get(\"flags\")),\n        }\n        return out\n\n    return obj\n\n# ---- run distill again ----\ndef load_teacher_outputs(path):\n    out = {}\n    if not path:\n        return out\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            r = json.loads(line)\n            out[r[\"id\"]] = r\n    return out\n\nteacher_outputs = {k: load_teacher_outputs(v) for k, v in teacher_paths.items() if v}\n\ndef router_priority(task):\n    if task in [\"receipt_extract_text\", \"invoice_extract_text\"]:\n        return [\"open_finance_8b\", \"finance_llama3_8b\", \"fingpt_lora_llama3_8b\"]\n    if task == \"journal_from_structured_txn\":\n        return [\"finance_llama3_8b\", \"open_finance_8b\", \"fingpt_lora_llama3_8b\"]\n    return [\"finance_llama3_8b\", \"open_finance_8b\", \"fingpt_lora_llama3_8b\"]\n\ndistilled = []\ndropped = 0\npicked = {}\n\nwith open(kd_pool_path, \"r\", encoding=\"utf-8\") as f:\n    pool = [json.loads(x) for x in f]\n\nfor ex in pool:\n    task = ex[\"task\"]\n    cid = ex[\"id\"]\n\n    chosen_obj = None\n    chosen_teacher = None\n\n    for tname in router_priority(task):\n        if tname not in teacher_outputs:\n            continue\n        rec = teacher_outputs[tname].get(cid)\n        if not rec:\n            continue\n\n        raw = rec.get(\"raw\", \"\")\n        obj = extract_json_robust(raw)\n        obj = coerce_to_schema(task, obj) if obj is not None else None\n\n        if obj is not None and schema_pass(task, obj):\n            chosen_obj = obj\n            chosen_teacher = tname\n            break\n\n    if chosen_obj is None:\n        dropped += 1\n        continue\n\n    picked[chosen_teacher] = picked.get(chosen_teacher, 0) + 1\n    distilled.append({\n        \"id\": cid,\n        \"task\": task,\n        \"prompt\": build_prompt(task, ex[\"input\"]),\n        \"answer_json\": chosen_obj\n    })\n\nprint(\"KD distilled:\", len(distilled), \"| dropped:\", dropped)\nprint(\"picked_by_teacher:\", picked)\n\ndistill_path = str(DATA_DIR / \"distilled_train.jsonl\")\nwith open(distill_path, \"w\", encoding=\"utf-8\") as f:\n    for r in distilled:\n        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n\nprint(\"Saved:\", distill_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:45:07.742224Z","iopub.execute_input":"2026-01-22T06:45:07.742392Z","iopub.status.idle":"2026-01-22T06:45:07.773047Z","shell.execute_reply.started":"2026-01-22T06:45:07.742375Z","shell.execute_reply":"2026-01-22T06:45:07.772559Z"}},"outputs":[{"name":"stdout","text":"KD distilled: 0 | dropped: 2000\npicked_by_teacher: {}\nSaved: /dev/shm/kaggle_ram/working/data/distilled_train.jsonl\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# CELL TRAIN (REPLACE) — bảo đảm lưu adapter vào 1 folder rõ ràng + tồn tại adapter_config.json\nfrom datasets import Dataset\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport json, os\nfrom pathlib import Path\n\nassert QWEN32B_PATH is not None, \"Cannot find Qwen3-32B in /kaggle/input\"\n\n# Load Qwen base\nqwen_tok, qwen_base = load_4bit_model(QWEN32B_PATH)\n\ndef guess_lora_targets(model):\n    names = set()\n    for n, _ in model.named_modules():\n        if any(k in n for k in [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]):\n            names.add(n.split(\".\")[-1])\n    return sorted(list(names)) if names else [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n\ntargets = guess_lora_targets(qwen_base)\nprint(\"LoRA targets:\", targets)\n\nlora_cfg = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=targets,\n)\n\nstudent = get_peft_model(qwen_base, lora_cfg)\nstudent.print_trainable_parameters()\n\n# Prepare dataset\nrows = []\nwith open(distill_path, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        r = json.loads(line)\n        rows.append({\"text\": r[\"prompt\"] + \"\\n\\n\" + json.dumps(r[\"answer_json\"], ensure_ascii=False)})\n\nif len(rows) == 0:\n    raise RuntimeError(\"distilled_train.jsonl is empty\")\n\ntrain_ds = Dataset.from_list(rows)\n\nargs = TrainingArguments(\n    output_dir=str(WORKDIR / \"student_ckpt\"),\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    learning_rate=1e-4,        # giảm từ 2e-4\n    num_train_epochs=2,        # chỉ tăng nếu B-STRICT tốt lên\n    warmup_steps=20,\n    logging_steps=10,\n    save_steps=200,\n    bf16=True,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",\n)\n\n\ntrainer = SFTTrainer(\n    model=student,\n    args=args,\n    train_dataset=train_ds,\n    processing_class=qwen_tok,   # TRL mới dùng processing_class\n)\n\ntrainer.train()\n\n# ✅ SAVE ADAPTER to a concrete path (and export ADAPTER_DIR for later cells)\nADAPTER_DIR = Path(WORKDIR) / \"outputs\" / \"adapters\" / \"student_adapter\"\nADAPTER_DIR.mkdir(parents=True, exist_ok=True)\n\ntrainer.model.save_pretrained(str(ADAPTER_DIR))\nqwen_tok.save_pretrained(str(ADAPTER_DIR))\n\n# hard assert: must exist\nassert (ADAPTER_DIR / \"adapter_config.json\").exists(), f\"Missing adapter_config.json in {ADAPTER_DIR}\"\nprint(\"✅ Saved student adapter:\", str(ADAPTER_DIR))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:45:07.773769Z","iopub.execute_input":"2026-01-22T06:45:07.773953Z","iopub.status.idle":"2026-01-22T06:45:36.262446Z","shell.execute_reply.started":"2026-01-22T06:45:07.773936Z","shell.execute_reply":"2026-01-22T06:45:36.261868Z"}},"outputs":[{"name":"stderr","text":"2026-01-22 06:45:13.946990: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769064314.425773     106 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769064314.554155     106 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769064315.687238     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769064315.687277     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769064315.687278     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769064315.687280     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_106/1546272563.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load Qwen base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mqwen_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqwen_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_4bit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQWEN32B_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mguess_lora_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'load_4bit_model' is not defined"],"ename":"NameError","evalue":"name 'load_4bit_model' is not defined","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"# =======================\n# CELL EVAL (STRICT GOLD, BF16) — score only MCOCR gold + NO default fill\n# Place: REPLACE your current CELL EVAL (after TRAIN)\n# =======================\nimport json, re, random\nfrom rapidfuzz import fuzz\n\ndef _strip_code_fences(s: str) -> str:\n    if not s:\n        return s\n    s = s.strip()\n    s = re.sub(r\"^```(?:json)?\\s*\", \"\", s, flags=re.I)\n    s = re.sub(r\"\\s*```$\", \"\", s)\n    return s.strip()\n\ndef _try_json_load(s: str):\n    try:\n        return json.loads(s)\n    except Exception:\n        return None\n\ndef extract_json_robust(text: str):\n    if text is None:\n        return None\n    t = _strip_code_fences(text)\n\n    obj = _try_json_load(t)\n    if obj is not None:\n        return obj\n\n    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", text, flags=re.S | re.I)\n    if m:\n        obj = _try_json_load(m.group(1))\n        if obj is not None:\n            return obj\n\n    for m in re.finditer(r\"\\{.*?\\}\", t, flags=re.S):\n        obj = _try_json_load(m.group(0))\n        if obj is not None:\n            return obj\n    return None\n\ndef _to_float(x):\n    if x is None:\n        return None\n    if isinstance(x, (int, float)):\n        return float(x)\n    s = str(x).replace(\",\", \".\")\n    s = re.sub(r\"[^0-9\\.\\-]\", \"\", s)\n    try:\n        return float(s) if s else None\n    except Exception:\n        return None\n\ndef _to_str(x):\n    if x is None:\n        return None\n    s = str(x).strip()\n    return s if s else None\n\ndef _to_flags(x):\n    if x is None:\n        return None\n    if isinstance(x, list):\n        return [str(v).strip() for v in x if str(v).strip()]\n    return [p.strip() for p in re.split(r\"[,;\\n]+\", str(x)) if p.strip()]\n\ndef coerce_receipt_strict(obj: dict):\n    \"\"\"\n    STRICT: NO DEFAULT FILL. Missing fields stay None -> schema should fail.\n    \"\"\"\n    if not isinstance(obj, dict):\n        return None\n    return {\n        \"vendor_name\": _to_str(obj.get(\"vendor_name\") or obj.get(\"vendor\") or obj.get(\"seller\")),\n        \"address\": _to_str(obj.get(\"address\") or obj.get(\"addr\")),\n        \"date\": _to_str(obj.get(\"date\")),\n        \"total_amount\": _to_float(obj.get(\"total_amount\") or obj.get(\"total\") or obj.get(\"amount\")),\n        \"currency\": _to_str(obj.get(\"currency\")),\n        \"confidence\": _to_float(obj.get(\"confidence\")),\n        \"flags\": _to_flags(obj.get(\"flags\")),\n    }\n\ndef vendor_similarity(a, b):\n    if not a or not b:\n        return None\n    return fuzz.token_set_ratio(str(a), str(b)) / 100.0\n\ndef amount_close(g, p, rel_tol=0.02, abs_tol=2000.0):\n    if g is None or p is None:\n        return None\n    try:\n        g = float(g); p = float(p)\n        return abs(g - p) <= max(abs_tol, rel_tol * max(abs(g), 1.0))\n    except Exception:\n        return None\n\ndef eval_receipt_gold_only(tok, mdl, cases, batch_size=32, max_new_tokens=256):\n    gold_cases = [\n        c for c in cases\n        if c.get(\"task\") == \"receipt_extract_text\"\n        and isinstance(c.get(\"gold\"), dict)\n        and any(c[\"gold\"].get(k) is not None for k in [\"vendor_name\",\"date\",\"total_amount\",\"address\"])\n    ]\n    n = len(gold_cases)\n    if n == 0:\n        return {\"n\": 0}\n\n    json_valid = 0\n    schema_ok = 0\n    vendor_sims = []\n    total_close = []\n    date_exact = []\n    bad = []\n\n    for i in range(0, n, batch_size):\n        batch = gold_cases[i:i+batch_size]\n        prompts = [build_prompt(\"receipt_extract_text\", ex[\"input\"]) for ex in batch]\n        texts = generate_batch(tok, mdl, prompts, max_new_tokens=max_new_tokens)\n\n        for ex, t in zip(batch, texts):\n            gold = ex[\"gold\"]\n\n            raw_obj = extract_json_robust(t)\n            obj = coerce_receipt_strict(raw_obj) if raw_obj else None\n\n            if obj is not None:\n                json_valid += 1\n                if schema_pass(\"receipt_extract_text\", obj):\n                    schema_ok += 1\n                else:\n                    if len(bad) < 10:\n                        bad.append({\"id\": ex[\"id\"], \"reason\": \"schema_fail\", \"raw\": t[:400]})\n            else:\n                if len(bad) < 10:\n                    bad.append({\"id\": ex[\"id\"], \"reason\": \"json_parse_fail\", \"raw\": t[:400]})\n\n            vs = vendor_similarity(gold.get(\"vendor_name\"), obj.get(\"vendor_name\") if obj else None)\n            if vs is not None:\n                vendor_sims.append(vs)\n\n            tc = amount_close(gold.get(\"total_amount\"), obj.get(\"total_amount\") if obj else None)\n            if tc is not None:\n                total_close.append(1.0 if tc else 0.0)\n\n            gd = gold.get(\"date\")\n            pd = obj.get(\"date\") if obj else None\n            if gd is not None and pd is not None:\n                date_exact.append(1.0 if str(gd).strip() == str(pd).strip() else 0.0)\n\n    def avg(xs):\n        return float(sum(xs)/len(xs)) if xs else None\n\n    return {\n        \"n\": n,\n        \"json_valid_rate\": json_valid / n,\n        \"schema_pass_rate\": schema_ok / n,\n        \"vendor_sim_avg\": avg(vendor_sims),\n        \"total_close_rate\": avg(total_close),\n        \"date_exact_rate\": avg(date_exact),\n        \"debug_bad_samples\": bad,\n    }\n\n# ---- load eval cases ----\nwith open(eval_path, \"r\", encoding=\"utf-8\") as f:\n    eval_cases = [json.loads(x) for x in f]\n\n# ---- BASE (BF16) ----\nbase_tok, base_mdl = load_bf16_model(QWEN32B_PATH)\nbase_gold = eval_receipt_gold_only(base_tok, base_mdl, eval_cases, batch_size=32, max_new_tokens=256)\nprint(\"BASE (GOLD receipt strict, BF16):\", {k: v for k, v in base_gold.items() if k != \"debug_bad_samples\"})\n\n# ---- STUDENT (BF16 base + adapter) ----\nfrom peft import PeftModel\nstudent_tok, student_base = load_bf16_model(QWEN32B_PATH)\nstudent_mdl = PeftModel.from_pretrained(student_base, str(ADAPTER_DIR))\nstudent_mdl.eval()\n\nstudent_gold = eval_receipt_gold_only(student_tok, student_mdl, eval_cases, batch_size=32, max_new_tokens=256)\nprint(\"STUDENT (GOLD receipt strict, BF16):\", {k: v for k, v in student_gold.items() if k != \"debug_bad_samples\"})\n\nreport = {\n    \"base_gold_receipt_strict_bf16\": base_gold,\n    \"student_gold_receipt_strict_bf16\": student_gold,\n    \"meta\": {\n        \"eval_path\": str(eval_path),\n        \"mode\": \"bf16_inference_only\",\n        \"task\": \"receipt_extract_text (gold only)\",\n        \"batch_size\": 32,\n        \"max_new_tokens\": 256,\n        \"notes\": \"strict: no default fill; score only gold-bearing MCOCR receipts\",\n    },\n}\n\nreport_path = str(DATA_DIR / \"eval_report_gold_strict_bf16.json\")\nwith open(report_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(report, f, ensure_ascii=False, indent=2)\n\nprint(\"Saved report:\", report_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:45:36.263053Z","iopub.status.idle":"2026-01-22T06:45:36.263236Z","shell.execute_reply.started":"2026-01-22T06:45:36.263145Z","shell.execute_reply":"2026-01-22T06:45:36.263157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FINAL SAVE (adapter) — student adapter in RAM\nADAPTER_DIR = str(WORKDIR / \"student_adapter\")\ntrainer.model.save_pretrained(ADAPTER_DIR)\nqwen_tok.save_pretrained(ADAPTER_DIR)\nprint(\"Saved student adapter dir:\", ADAPTER_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:45:36.264047Z","iopub.status.idle":"2026-01-22T06:45:36.264233Z","shell.execute_reply.started":"2026-01-22T06:45:36.264146Z","shell.execute_reply":"2026-01-22T06:45:36.264157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL SAVE (ADD NEW, cuối notebook) — xuất student adapter + tokenizer + lineage để tải về\nimport json, shutil\nfrom pathlib import Path\n\nOUT_DIR = WORKDIR / \"outputs\"\nREL = Path(\"/kaggle/working\") / \"release\" / \"qwen3-32b-accounting-distilled-v0.1.0\"\nREL.mkdir(parents=True, exist_ok=True)\n\nADAPTER_OUT = REL / \"adapters\"\nTOKEN_OUT  = REL / \"tokenizer\"\nADAPTER_OUT.mkdir(parents=True, exist_ok=True)\nTOKEN_OUT.mkdir(parents=True, exist_ok=True)\n\n# Save LoRA adapter (nhẹ) + tokenizer\ntrainer.model.save_pretrained(str(ADAPTER_OUT))\nqwen_tok.save_pretrained(str(TOKEN_OUT))\n\nlineage = {\n    \"model_name\": \"qwen3-32b-accounting-distilled\",\n    \"version\": \"v0.1.0\",\n    \"method\": \"Knowledge Distillation (multi-teacher) + QLoRA SFT\",\n    \"teachers\": TEACHERS,\n    \"paths\": {\n        \"distill_path\": str(distill_path),\n        \"adapter_dir\": str(ADAPTER_OUT),\n        \"tokenizer_dir\": str(TOKEN_OUT),\n    },\n}\nwith open(REL / \"lineage.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(lineage, f, ensure_ascii=False, indent=2)\n\n# Zip để download dễ\nzip_path = shutil.make_archive(str(REL), \"zip\", root_dir=str(REL))\nprint(\"✅ RELEASE folder:\", REL)\nprint(\"✅ ZIP:\", zip_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T06:45:36.266941Z","iopub.status.idle":"2026-01-22T06:45:36.267164Z","shell.execute_reply.started":"2026-01-22T06:45:36.267069Z","shell.execute_reply":"2026-01-22T06:45:36.267081Z"}},"outputs":[],"execution_count":null}]}