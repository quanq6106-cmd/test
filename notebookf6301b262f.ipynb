{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":3424966,"sourceType":"datasetVersion","datasetId":2035671},{"sourceId":3818163,"sourceType":"datasetVersion","datasetId":2274483},{"sourceId":4501486,"sourceType":"datasetVersion","datasetId":2631784},{"sourceId":5258124,"sourceType":"datasetVersion","datasetId":3059801},{"sourceId":9971715,"sourceType":"datasetVersion","datasetId":6134857},{"sourceId":11087772,"sourceType":"datasetVersion","datasetId":4576291},{"sourceId":11497644,"sourceType":"datasetVersion","datasetId":7207743},{"sourceId":11564740,"sourceType":"datasetVersion","datasetId":7251054},{"sourceId":11739593,"sourceType":"datasetVersion","datasetId":5773627},{"sourceId":363168,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":301540,"modelId":322000}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install -U transformers accelerate datasets peft trl bitsandbytes jsonschema rapidfuzz huggingface_hub openpyxl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T02:46:23.002428Z","iopub.execute_input":"2026-01-21T02:46:23.002686Z","iopub.status.idle":"2026-01-21T02:46:37.672967Z","shell.execute_reply.started":"2026-01-21T02:46:23.002665Z","shell.execute_reply":"2026-01-21T02:46:37.672421Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.5/532.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nray 2.52.1 requires click!=8.3.*,>=7.0, but you have click 8.3.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os, re, json, time, math, random\nfrom pathlib import Path\nfrom glob import glob\n\nWORKDIR = Path(\"/kaggle/working\")\nDATA_DIR = WORKDIR / \"data\"\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\n# ===== Auto-discover Qwen3-32B model path in /kaggle/input =====\ndef find_qwen32b_path():\n    candidates = []\n    for p in glob(\"/kaggle/input/**\", recursive=True):\n        if os.path.isdir(p):\n            low = p.lower()\n            if \"qwen\" in low and (\"32b\" in low or \"32-b\" in low):\n                # must contain config.json to be HF-compatible\n                if os.path.exists(os.path.join(p, \"config.json\")):\n                    candidates.append(p)\n    # choose shortest path (usually the root model folder)\n    candidates = sorted(candidates, key=lambda x: len(x))\n    return candidates[0] if candidates else None\n\nQWEN32B_PATH = find_qwen32b_path()\nprint(\"QWEN32B_PATH =\", QWEN32B_PATH)\n\n# ===== Teachers (HF) =====\nTEACHERS = {\n    \"open_finance_8b\": \"DragonLLM/Llama-Open-Finance-8B\",            # gated :contentReference[oaicite:3]{index=3}\n    \"finance_llama3_8b\": \"instruction-pretrain/finance-Llama3-8B\",   # public :contentReference[oaicite:4]{index=4}\n    \"fingpt_lora_llama3_8b\": \"FinGPT/fingpt-mt_llama3-8b_lora\"        # adapter :contentReference[oaicite:5]{index=5}\n}\nFINGPT_BASE = \"meta-llama/Meta-Llama-3-8B\"  # gated sometimes :contentReference[oaicite:6]{index=6}\n\n# ===== Dataset roots (bạn đã mount sẵn) =====\nPATHS = {\n    \"vn_mcocr\": \"/kaggle/input/vietnamese-receipts-mc-ocr-2021\",\n    \"invoice_ocr\": \"/kaggle/input/invoice-ocr\",\n    \"hi_quality_invoice\": \"/kaggle/input/high-quality-invoice-images-for-ocr\",\n    \"gl_xlsx\": \"/kaggle/input/generalledger/Data file for students.xlsx\",\n    \"transactions_csv\": \"/kaggle/input/financial-transactions-dataset/financial_transactions.csv\",\n    \"forecast_csv\": \"/kaggle/input/financial-forecasting-data/simulated_financial_forecasting_data.csv\",\n    \"data_retriever_csv\": \"/kaggle/input/data-retreiver/Data_ret.csv\",\n}\nprint(\"DATA PATHS OK:\", PATHS)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T02:46:37.673961Z","iopub.execute_input":"2026-01-21T02:46:37.674103Z","iopub.status.idle":"2026-01-21T02:50:51.032469Z","shell.execute_reply.started":"2026-01-21T02:46:37.674087Z","shell.execute_reply":"2026-01-21T02:50:51.032051Z"}},"outputs":[{"name":"stdout","text":"QWEN32B_PATH = /kaggle/input/qwen-3/transformers/32b/1\nDATA PATHS OK: {'vn_mcocr': '/kaggle/input/vietnamese-receipts-mc-ocr-2021', 'invoice_ocr': '/kaggle/input/invoice-ocr', 'hi_quality_invoice': '/kaggle/input/high-quality-invoice-images-for-ocr', 'gl_xlsx': '/kaggle/input/generalledger/Data file for students.xlsx', 'transactions_csv': '/kaggle/input/financial-transactions-dataset/financial_transactions.csv', 'forecast_csv': '/kaggle/input/financial-forecasting-data/simulated_financial_forecasting_data.csv', 'data_retriever_csv': '/kaggle/input/data-retreiver/Data_ret.csv'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from jsonschema import validate\nfrom jsonschema.exceptions import ValidationError\n\n# ===== Schemas =====\nRECEIPT_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"vendor_name\": {\"type\": [\"string\", \"null\"]},\n        \"address\": {\"type\": [\"string\", \"null\"]},\n        \"date\": {\"type\": [\"string\", \"null\"]},            # YYYY-MM-DD preferred\n        \"total_amount\": {\"type\": [\"number\", \"null\"]},\n        \"currency\": {\"type\": [\"string\", \"null\"]},        # \"VND\"\n        \"confidence\": {\"type\": \"number\"},\n        \"flags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n    },\n    \"required\": [\"vendor_name\",\"address\",\"date\",\"total_amount\",\"currency\",\"confidence\",\"flags\"]\n}\n\nINVOICE_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"vendor_name\": {\"type\": [\"string\", \"null\"]},\n        \"invoice_no\": {\"type\": [\"string\", \"null\"]},\n        \"date\": {\"type\": [\"string\", \"null\"]},\n        \"subtotal\": {\"type\": [\"number\", \"null\"]},\n        \"tax\": {\"type\": [\"number\", \"null\"]},\n        \"total\": {\"type\": [\"number\", \"null\"]},\n        \"currency\": {\"type\": [\"string\", \"null\"]},\n        \"confidence\": {\"type\": \"number\"},\n        \"flags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n    },\n    \"required\": [\"vendor_name\",\"invoice_no\",\"date\",\"subtotal\",\"tax\",\"total\",\"currency\",\"confidence\",\"flags\"]\n}\n\nJOURNAL_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"entries\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"account\": {\"type\": \"string\"},\n                    \"debit\": {\"type\": \"number\"},\n                    \"credit\": {\"type\": \"number\"},\n                    \"memo\": {\"type\": [\"string\",\"null\"]}\n                },\n                \"required\": [\"account\",\"debit\",\"credit\",\"memo\"]\n            }\n        },\n        \"confidence\": {\"type\": \"number\"},\n        \"flags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n    },\n    \"required\": [\"entries\",\"confidence\",\"flags\"]\n}\n\nTASK2SCHEMA = {\n    \"receipt_extract_text\": RECEIPT_SCHEMA,\n    \"invoice_extract_text\": INVOICE_SCHEMA,\n    \"journal_from_structured_txn\": JOURNAL_SCHEMA,\n}\n\ndef schema_pass(task: str, obj: dict) -> bool:\n    try:\n        validate(instance=obj, schema=TASK2SCHEMA[task])\n        return True\n    except ValidationError:\n        return False\n    except Exception:\n        return False\n\n# ===== JSON extract / repair =====\ndef extract_json_from_text(text: str):\n    if text is None:\n        return None\n    # pick first {...} block\n    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n    if not m:\n        return None\n    s = m.group(0)\n    try:\n        return json.loads(s)\n    except Exception:\n        return None\n\ndef json_repair_minimal(text: str):\n    \"\"\"\n    deterministic repair for common LLM issues:\n    - trailing commas\n    - single quotes -> double quotes (simple cases)\n    \"\"\"\n    if text is None:\n        return None\n    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n    if not m:\n        return None\n    s = m.group(0).strip()\n\n    s = re.sub(r\",\\s*}\", \"}\", s)\n    s = re.sub(r\",\\s*]\", \"]\", s)\n    # naive quote fix (only if it looks like JSON)\n    if \"'\" in s and '\"' not in s:\n        s = s.replace(\"'\", '\"')\n\n    try:\n        return json.loads(s)\n    except Exception:\n        return None\n\n# ===== Prompt builder =====\ndef build_prompt(task: str, input_data):\n    if task == \"receipt_extract_text\":\n        return f\"\"\"\nExtract receipt key fields from Vietnamese text.\nReturn ONLY valid JSON with fields:\nvendor_name,address,date,total_amount,currency,confidence,flags\n\nReceipt Text:\n{input_data}\n\"\"\".strip()\n\n    if task == \"invoice_extract_text\":\n        return f\"\"\"\nExtract invoice fields from text.\nReturn ONLY valid JSON with fields:\nvendor_name,invoice_no,date,subtotal,tax,total,currency,confidence,flags\n\nInvoice Text:\n{input_data}\n\"\"\".strip()\n\n    if task == \"journal_from_structured_txn\":\n        return f\"\"\"\nYou are an ERP accountant.\nGiven a structured transaction JSON, propose journal entries.\nReturn ONLY valid JSON with fields:\nentries[{account,debit,credit,memo}],confidence,flags\n\nTransaction:\n{json.dumps(input_data, ensure_ascii=False)}\n\"\"\".strip()\n\n    raise ValueError(\"Unknown task\")\n\nprint(\"Schemas + prompt builder ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T02:51:41.472203Z","iopub.execute_input":"2026-01-21T02:51:41.472433Z","iopub.status.idle":"2026-01-21T02:51:42.347267Z","shell.execute_reply.started":"2026-01-21T02:51:41.472416Z","shell.execute_reply":"2026-01-21T02:51:42.346824Z"}},"outputs":[{"name":"stdout","text":"Schemas + prompt builder ready.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\ndef infer_col(df, candidates):\n    cols = {c.lower(): c for c in df.columns}\n    for cand in candidates:\n        if cand.lower() in cols:\n            return cols[cand.lower()]\n    # fuzzy contains\n    for c in df.columns:\n        low = c.lower()\n        for cand in candidates:\n            if cand.lower() in low:\n                return c\n    return None\n\ndef load_vn_mcocr_cases(limit=300):\n    root = PATHS[\"vn_mcocr\"]\n    cases = []\n\n    # Prefer CSV with gold labels if present\n    csv_candidates = [\n        os.path.join(root, \"mcocr_train_df.csv\"),\n        os.path.join(root, \"mcocr_val_sample_df.csv\"),\n        os.path.join(root, \"results.csv\"),\n    ]\n    for p in csv_candidates:\n        if os.path.exists(p):\n            df = pd.read_csv(p)\n\n            text_col = infer_col(df, [\"text\", \"ocr_text\", \"raw_text\", \"content\", \"transcription\"])\n            if text_col is None:\n                # fallback pick longest string col\n                str_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n                if str_cols:\n                    text_col = max(str_cols, key=lambda c: df[c].astype(str).str.len().mean())\n\n            seller_col = infer_col(df, [\"seller\", \"vendor\", \"vendor_name\", \"merchant\", \"store\", \"shop\"])\n            addr_col   = infer_col(df, [\"address\", \"seller_address\", \"vendor_address\"])\n            date_col   = infer_col(df, [\"timestamp\", \"date\", \"datetime\", \"time\"])\n            total_col  = infer_col(df, [\"total_cost\", \"total\", \"amount\", \"total_amount\", \"sum\"])\n\n            for i, row in df.head(limit).iterrows():\n                raw_text = str(row[text_col]) if text_col else \"\"\n\n                gold = None\n                if seller_col or addr_col or date_col or total_col:\n                    def safe_float(x):\n                        try:\n                            if pd.isna(x): \n                                return None\n                            s = str(x)\n                            s = re.sub(r\"[^\\d\\.\\-]\", \"\", s)\n                            return float(s) if s else None\n                        except:\n                            return None\n\n                    gold = {\n                        \"vendor_name\": str(row[seller_col]) if seller_col and pd.notna(row[seller_col]) else None,\n                        \"address\": str(row[addr_col]) if addr_col and pd.notna(row[addr_col]) else None,\n                        \"date\": str(row[date_col]) if date_col and pd.notna(row[date_col]) else None,\n                        \"total_amount\": safe_float(row[total_col]) if total_col else None,\n                        \"currency\": \"VND\",\n                        \"confidence\": 0.0,\n                        \"flags\": []\n                    }\n\n                cases.append({\n                    \"id\": f\"vn_mcocr_{i}\",\n                    \"task\": \"receipt_extract_text\",\n                    \"input\": raw_text,\n                    \"gold\": gold,\n                    \"meta\": {\"source\": os.path.basename(p)}\n                })\n            return cases\n\n    # fallback txt (OCR lines)\n    txt_candidates = [\n        os.path.join(root, \"text_recognition_train_data.txt\"),\n        os.path.join(root, \"text_recognition_val_data.txt\"),\n    ]\n    for p in txt_candidates:\n        if os.path.exists(p):\n            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                for idx, line in enumerate(f):\n                    if idx >= limit:\n                        break\n                    parts = line.strip().split(\"\\t\")\n                    raw_text = parts[-1] if parts else \"\"\n                    cases.append({\n                        \"id\": f\"vn_mcocr_txt_{idx}\",\n                        \"task\": \"receipt_extract_text\",\n                        \"input\": raw_text,\n                        \"gold\": None,\n                        \"meta\": {\"source\": os.path.basename(p)}\n                    })\n            return cases\n\n    return []\n\ndef load_gl_cases(limit=200):\n    xlsx_path = PATHS[\"gl_xlsx\"]\n    if not os.path.exists(xlsx_path):\n        return []\n    xls = pd.ExcelFile(xlsx_path)\n    # take first sheet by default\n    df = pd.read_excel(xlsx_path, sheet_name=xls.sheet_names[0])\n\n    cases = []\n    for i, row in df.head(limit).iterrows():\n        txn = row.to_dict()\n        cases.append({\n            \"id\": f\"gl_{i}\",\n            \"task\": \"journal_from_structured_txn\",\n            \"input\": txn,\n            \"gold\": None,\n            \"meta\": {\"sheet\": xls.sheet_names[0]}\n        })\n    return cases\n\ndef load_invoice_ocr_cases(limit=200):\n    \"\"\"\n    Robust loader:\n    - If JSON/CSV annotations exist -> use their text fields\n    - Otherwise use image paths (text-only LLM can't read images, but still valid for KD if you later OCR)\n    \"\"\"\n    root = PATHS[\"invoice_ocr\"]\n    if not os.path.exists(root):\n        return []\n\n    ann_files = []\n    for ext in [\"*.json\",\"*.csv\"]:\n        ann_files += glob(os.path.join(root, \"**\", ext), recursive=True)\n\n    cases = []\n    if ann_files:\n        # take first annotation file found\n        p = ann_files[0]\n        if p.endswith(\".csv\"):\n            df = pd.read_csv(p)\n            text_col = infer_col(df, [\"text\",\"ocr\",\"raw\",\"content\"])\n            for i, row in df.head(limit).iterrows():\n                raw_text = str(row[text_col]) if text_col else \"\"\n                cases.append({\n                    \"id\": f\"invoice_ocr_csv_{i}\",\n                    \"task\": \"invoice_extract_text\",\n                    \"input\": raw_text,\n                    \"gold\": None,\n                    \"meta\": {\"ann\": os.path.basename(p)}\n                })\n        else:\n            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                js = json.load(f)\n            # try to find list items with \"text\"\n            items = []\n            if isinstance(js, list):\n                items = js\n            elif isinstance(js, dict):\n                # common keys\n                for k in [\"data\",\"items\",\"annotations\",\"samples\"]:\n                    if k in js and isinstance(js[k], list):\n                        items = js[k]\n                        break\n\n            for i, it in enumerate(items[:limit]):\n                raw_text = it.get(\"text\") or it.get(\"ocr_text\") or it.get(\"content\") or \"\"\n                cases.append({\n                    \"id\": f\"invoice_ocr_json_{i}\",\n                    \"task\": \"invoice_extract_text\",\n                    \"input\": str(raw_text),\n                    \"gold\": None,\n                    \"meta\": {\"ann\": os.path.basename(p)}\n                })\n\n        return cases\n\n    # fallback: use image paths (for later OCR pipeline)\n    imgs = glob(os.path.join(root, \"**\", \"*.png\"), recursive=True) + glob(os.path.join(root, \"**\", \"*.jpg\"), recursive=True)\n    for i, ip in enumerate(imgs[:limit]):\n        cases.append({\n            \"id\": f\"invoice_ocr_img_{i}\",\n            \"task\": \"invoice_extract_text\",\n            \"input\": f\"[IMAGE_PATH]{ip}\",\n            \"gold\": None,\n            \"meta\": {\"img\": os.path.basename(ip)}\n        })\n    return cases\n\nprint(\"Loaders ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T02:51:42.348154Z","iopub.execute_input":"2026-01-21T02:51:42.348300Z","iopub.status.idle":"2026-01-21T02:51:42.458686Z","shell.execute_reply.started":"2026-01-21T02:51:42.348286Z","shell.execute_reply":"2026-01-21T02:51:42.458057Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_106/2251941367.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minfer_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcand\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"],"ename":"ValueError","evalue":"numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"def write_jsonl(path, rows):\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        for r in rows:\n            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n\neval_cases = []\neval_cases += load_vn_mcocr_cases(limit=300)\neval_cases += load_invoice_ocr_cases(limit=200)\neval_cases += load_gl_cases(limit=150)\n\nprint(\"Total eval cases:\", len(eval_cases))\neval_path = str(DATA_DIR / \"eval_cases.jsonl\")\nwrite_jsonl(eval_path, eval_cases)\nprint(\"Saved:\", eval_path)\n\n# KD training uses the same pool (you can enlarge later)\nkd_pool = eval_cases.copy()\nrandom.shuffle(kd_pool)\nkd_pool = kd_pool[:500]  # keep KD small for iteration speed\nkd_pool_path = str(DATA_DIR / \"kd_pool.jsonl\")\nwrite_jsonl(kd_pool_path, kd_pool)\nprint(\"Saved KD pool:\", kd_pool_path, \"| size:\", len(kd_pool))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T02:51:42.459081Z","iopub.status.idle":"2026-01-21T02:51:42.459250Z","shell.execute_reply.started":"2026-01-21T02:51:42.459171Z","shell.execute_reply":"2026-01-21T02:51:42.459180Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\nbnb4 = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\ndef load_4bit_model(repo_or_path: str):\n    tok = AutoTokenizer.from_pretrained(repo_or_path, use_fast=True, trust_remote_code=True)\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n    tok.padding_side = \"left\"\n\n    mdl = AutoModelForCausalLM.from_pretrained(\n        repo_or_path,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16,\n        quantization_config=bnb4,\n        trust_remote_code=True,\n        attn_implementation=\"sdpa\",\n    )\n    mdl.eval()\n    return tok, mdl\n\ndef load_fingpt(adapter_repo: str, base_repo: str):\n    tok, base = load_4bit_model(base_repo)\n    mdl = PeftModel.from_pretrained(base, adapter_repo)\n    mdl.eval()\n    return tok, mdl\n\n@torch.no_grad()\ndef generate_batch(tok, mdl, prompts, max_new_tokens=320):\n    enc = tok(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(mdl.device)\n    out = mdl.generate(\n        **enc,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        temperature=0.0,\n        top_p=1.0,\n        repetition_penalty=1.05,\n    )\n    return tok.batch_decode(out, skip_special_tokens=True)\n\nprint(\"Model utils ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T02:51:42.459881Z","iopub.status.idle":"2026-01-21T02:51:42.460015Z","shell.execute_reply.started":"2026-01-21T02:51:42.459948Z","shell.execute_reply":"2026-01-21T02:51:42.459956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\nHF_TOKEN = os.getenv(\"HF_TOKEN\", None)\nif HF_TOKEN:\n    login(token=HF_TOKEN)\n    print(\"HF login OK\")\nelse:\n    print(\"HF_TOKEN missing -> gated teachers may fail (OpenFinance/Llama base).\")\n\nTEACH_CACHE_DIR = DATA_DIR / \"teacher_outputs\"\nTEACH_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n\ndef run_teacher(name, repo=None, adapter=None, base=None, batch_size=16):\n    print(f\"\\n=== Teacher: {name} ===\")\n    try:\n        if adapter and base:\n            tok, mdl = load_fingpt(adapter, base)\n        else:\n            tok, mdl = load_4bit_model(repo)\n\n        # load KD pool\n        with open(kd_pool_path, \"r\", encoding=\"utf-8\") as f:\n            pool = [json.loads(x) for x in f]\n\n        out_path = TEACH_CACHE_DIR / f\"{name}.jsonl\"\n        with open(out_path, \"w\", encoding=\"utf-8\") as fw:\n            for i in range(0, len(pool), batch_size):\n                batch = pool[i:i+batch_size]\n                prompts = [build_prompt(ex[\"task\"], ex[\"input\"]) for ex in batch]\n                texts = generate_batch(tok, mdl, prompts)\n\n                for ex, t in zip(batch, texts):\n                    fw.write(json.dumps({\n                        \"id\": ex[\"id\"],\n                        \"task\": ex[\"task\"],\n                        \"raw\": t\n                    }, ensure_ascii=False) + \"\\n\")\n\n        del mdl\n        torch.cuda.empty_cache()\n        print(\"Saved:\", str(out_path))\n        return str(out_path)\n    except Exception as e:\n        print(\"FAILED:\", name, \"| reason:\", str(e))\n        return None\n\nteacher_paths = {}\n\n# finance llama3 (public)\nteacher_paths[\"finance_llama3_8b\"] = run_teacher(\"finance_llama3_8b\", repo=TEACHERS[\"finance_llama3_8b\"])\n\n# open finance (gated)\nteacher_paths[\"open_finance_8b\"] = run_teacher(\"open_finance_8b\", repo=TEACHERS[\"open_finance_8b\"])\n\n# fingpt lora (needs base llama3 gated)\nteacher_paths[\"fingpt_lora_llama3_8b\"] = run_teacher(\n    \"fingpt_lora_llama3_8b\",\n    adapter=TEACHERS[\"fingpt_lora_llama3_8b\"],\n    base=FINGPT_BASE\n)\n\nprint(\"\\nTeacher paths:\", teacher_paths)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T02:51:42.460403Z","iopub.status.idle":"2026-01-21T02:51:42.460536Z","shell.execute_reply.started":"2026-01-21T02:51:42.460463Z","shell.execute_reply":"2026-01-21T02:51:42.460470Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_teacher_outputs(path):\n    out = {}\n    if not path:\n        return out\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            r = json.loads(line)\n            out[r[\"id\"]] = r\n    return out\n\nteacher_outputs = {k: load_teacher_outputs(v) for k,v in teacher_paths.items() if v}\n\ndef router_priority(task):\n    if task in [\"receipt_extract_text\", \"invoice_extract_text\"]:\n        return [\"open_finance_8b\", \"finance_llama3_8b\", \"fingpt_lora_llama3_8b\"]\n    if task == \"journal_from_structured_txn\":\n        return [\"finance_llama3_8b\", \"open_finance_8b\", \"fingpt_lora_llama3_8b\"]\n    return [\"finance_llama3_8b\", \"open_finance_8b\", \"fingpt_lora_llama3_8b\"]\n\ndistilled = []\ndropped = 0\npicked = {}\n\nwith open(kd_pool_path, \"r\", encoding=\"utf-8\") as f:\n    pool = [json.loads(x) for x in f]\n\nfor ex in pool:\n    task = ex[\"task\"]\n    cid  = ex[\"id\"]\n\n    chosen_obj = None\n    chosen_teacher = None\n\n    for tname in router_priority(task):\n        if tname not in teacher_outputs:\n            continue\n        rec = teacher_outputs[tname].get(cid)\n        if not rec:\n            continue\n\n        raw = rec[\"raw\"]\n        obj = extract_json_from_text(raw)\n        if obj is None:\n            obj = json_repair_minimal(raw)\n\n        if obj is not None and schema_pass(task, obj):\n            chosen_obj = obj\n            chosen_teacher = tname\n            break\n\n    if chosen_obj is None:\n        dropped += 1\n        continue\n\n    picked[chosen_teacher] = picked.get(chosen_teacher, 0) + 1\n    distilled.append({\n        \"id\": cid,\n        \"task\": task,\n        \"prompt\": build_prompt(task, ex[\"input\"]),\n        \"answer_json\": chosen_obj\n    })\n\nprint(\"KD distilled:\", len(distilled), \"| dropped:\", dropped)\nprint(\"picked_by_teacher:\", picked)\n\ndistill_path = str(DATA_DIR / \"distilled_train.jsonl\")\nwith open(distill_path, \"w\", encoding=\"utf-8\") as f:\n    for r in distilled:\n        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n\nprint(\"Saved:\", distill_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T02:51:42.461122Z","iopub.status.idle":"2026-01-21T02:51:42.461248Z","shell.execute_reply.started":"2026-01-21T02:51:42.461186Z","shell.execute_reply":"2026-01-21T02:51:42.461194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n\nassert QWEN32B_PATH is not None, \"Cannot find Qwen3-32B in /kaggle/input\"\n\n# Load Qwen base\nqwen_tok, qwen_base = load_4bit_model(QWEN32B_PATH)\n\n# Auto-target modules (robust across architectures)\ndef guess_lora_targets(model):\n    names = set()\n    for n, _ in model.named_modules():\n        if any(k in n for k in [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]):\n            names.add(n.split(\".\")[-1])\n    # fallback default\n    if not names:\n        return [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n    return sorted(list(names))\n\ntargets = guess_lora_targets(qwen_base)\nprint(\"LoRA targets:\", targets)\n\nlora_cfg = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=targets\n)\n\nstudent = get_peft_model(qwen_base, lora_cfg)\nstudent.print_trainable_parameters()\n\n# Prepare dataset\nrows = []\nwith open(distill_path, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        r = json.loads(line)\n        rows.append({\n            \"text\": r[\"prompt\"] + \"\\n\\n\" + json.dumps(r[\"answer_json\"], ensure_ascii=False)\n        })\n\ntrain_ds = Dataset.from_list(rows)\n\nargs = TrainingArguments(\n    output_dir=str(WORKDIR / \"student_ckpt\"),\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    num_train_epochs=1,\n    logging_steps=10,\n    save_steps=200,\n    bf16=True,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\"\n)\n\ntrainer = SFTTrainer(\n    model=student,\n    tokenizer=qwen_tok,\n    train_dataset=train_ds,\n    args=args,\n    max_seq_length=2048,\n)\n\ntrainer.train()\n\nADAPTER_DIR = str(WORKDIR / \"student_adapter\")\ntrainer.model.save_pretrained(ADAPTER_DIR)\nqwen_tok.save_pretrained(ADAPTER_DIR)\n\nprint(\"Saved student adapter:\", ADAPTER_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T02:51:42.461717Z","iopub.status.idle":"2026-01-21T02:51:42.461856Z","shell.execute_reply.started":"2026-01-21T02:51:42.461784Z","shell.execute_reply":"2026-01-21T02:51:42.461793Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize_obj(obj):\n    if obj is None:\n        return None\n    # stable dump for determinism comparison\n    return json.dumps(obj, ensure_ascii=False, sort_keys=True)\n\ndef field_exact(gold, pred, key):\n    if gold is None or pred is None:\n        return None\n    if gold.get(key) is None:\n        return None\n    return 1.0 if str(gold.get(key)).strip() == str(pred.get(key)).strip() else 0.0\n\ndef eval_model(tok, mdl, cases, repeats=3, batch_size=8):\n    stats = {\n        \"json_valid_rate\": 0,\n        \"schema_pass_rate\": 0,\n        \"determinism_rate\": 0,\n        \"n\": len(cases),\n        \"field_vendor_acc\": [],\n        \"field_total_acc\": [],\n        \"field_date_acc\": [],\n    }\n\n    det_same = 0\n    valid = 0\n    schema_ok = 0\n\n    for i in range(0, len(cases), batch_size):\n        batch = cases[i:i+batch_size]\n        prompts = [build_prompt(ex[\"task\"], ex[\"input\"]) for ex in batch]\n\n        # determinism: run repeats times\n        outputs_all = []\n        for _ in range(repeats):\n            texts = generate_batch(tok, mdl, prompts, max_new_tokens=320)\n            objs = []\n            for ex, t in zip(batch, texts):\n                obj = extract_json_from_text(t) or json_repair_minimal(t)\n                objs.append(obj)\n            outputs_all.append(objs)\n\n        # compute per-sample stats\n        for j, ex in enumerate(batch):\n            task = ex[\"task\"]\n            gold = ex.get(\"gold\")\n\n            # use first run as \"pred\"\n            pred = outputs_all[0][j]\n\n            if pred is not None:\n                valid += 1\n                if schema_pass(task, pred):\n                    schema_ok += 1\n\n            # determinism check: all normalized equal\n            norms = [normalize_obj(outputs_all[r][j]) for r in range(repeats)]\n            if len(set(norms)) == 1:\n                det_same += 1\n\n            # Tier B field acc if gold exists & relevant\n            if gold and isinstance(gold, dict) and task in [\"receipt_extract_text\",\"invoice_extract_text\"]:\n                v = field_exact(gold, pred, \"vendor_name\")\n                d = field_exact(gold, pred, \"date\")\n                # receipt: total_amount ; invoice: total\n                if task == \"receipt_extract_text\":\n                    tacc = field_exact(gold, pred, \"total_amount\")\n                else:\n                    tacc = field_exact(gold, pred, \"total\")\n\n                if v is not None: stats[\"field_vendor_acc\"].append(v)\n                if d is not None: stats[\"field_date_acc\"].append(d)\n                if tacc is not None: stats[\"field_total_acc\"].append(tacc)\n\n    n = max(1, stats[\"n\"])\n    stats[\"json_valid_rate\"] = valid / n\n    stats[\"schema_pass_rate\"] = schema_ok / n\n    stats[\"determinism_rate\"] = det_same / n\n\n    def avg(x):\n        return float(sum(x)/len(x)) if x else None\n\n    stats[\"vendor_acc\"] = avg(stats[\"field_vendor_acc\"])\n    stats[\"date_acc\"] = avg(stats[\"field_date_acc\"])\n    stats[\"total_acc\"] = avg(stats[\"field_total_acc\"])\n\n    # cleanup arrays\n    stats.pop(\"field_vendor_acc\", None)\n    stats.pop(\"field_date_acc\", None)\n    stats.pop(\"field_total_acc\", None)\n\n    return stats\n\n# Load eval cases\nwith open(eval_path, \"r\", encoding=\"utf-8\") as f:\n    eval_cases = [json.loads(x) for x in f]\n\n# BASE QWEN\nbase_tok, base_mdl = load_4bit_model(QWEN32B_PATH)\nbase_stats = eval_model(base_tok, base_mdl, eval_cases, repeats=3)\nprint(\"BASE:\", base_stats)\n\n# STUDENT = base + adapter\nfrom peft import PeftModel\nstudent_tok, student_base = load_4bit_model(QWEN32B_PATH)\nstudent_mdl = PeftModel.from_pretrained(student_base, ADAPTER_DIR)\nstudent_mdl.eval()\nstudent_stats = eval_model(student_tok, student_mdl, eval_cases, repeats=3)\nprint(\"STUDENT:\", student_stats)\n\nreport = {\n    \"base_qwen3_32b\": base_stats,\n    \"student_qwen3_32b_adapter\": student_stats,\n}\n\nreport_path = str(DATA_DIR / \"eval_report.json\")\nwith open(report_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(report, f, ensure_ascii=False, indent=2)\n\nprint(\"Saved report:\", report_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T02:51:42.462256Z","iopub.status.idle":"2026-01-21T02:51:42.462386Z","shell.execute_reply.started":"2026-01-21T02:51:42.462320Z","shell.execute_reply":"2026-01-21T02:51:42.462328Z"}},"outputs":[],"execution_count":null}]}