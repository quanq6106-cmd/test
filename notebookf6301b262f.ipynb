{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaH100","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":3424966,"sourceType":"datasetVersion","datasetId":2035671},{"sourceId":3818163,"sourceType":"datasetVersion","datasetId":2274483},{"sourceId":4501486,"sourceType":"datasetVersion","datasetId":2631784},{"sourceId":5258124,"sourceType":"datasetVersion","datasetId":3059801},{"sourceId":9971715,"sourceType":"datasetVersion","datasetId":6134857},{"sourceId":11087772,"sourceType":"datasetVersion","datasetId":4576291},{"sourceId":11497644,"sourceType":"datasetVersion","datasetId":7207743},{"sourceId":11564740,"sourceType":"datasetVersion","datasetId":7251054},{"sourceId":11739593,"sourceType":"datasetVersion","datasetId":5773627},{"sourceId":363168,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":301540,"modelId":322000}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELL 2 (REPLACE) — install xong KHÔNG import transformers ở đây (để tránh cache về disk trước)\n!pip -q install -U --no-cache-dir \\\n  \"transformers>=4.51.0\" \\\n  \"accelerate>=0.30.0\" \\\n  \"datasets>=2.19.0\" \\\n  \"peft>=0.11.0\" \\\n  \"trl>=0.9.6\" \\\n  \"bitsandbytes>=0.43.1\" \\\n  \"huggingface_hub>=0.23.0\" \\\n  \"tokenizers>=0.21.0\" \\\n  \"safetensors>=0.4.3\" \\\n  \"sentencepiece\" \\\n  \"jsonschema>=4.22.0\" \\\n  \"rapidfuzz>=3.9.0\" \\\n  \"openpyxl>=3.1.5\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T06:53:16.329548Z","iopub.execute_input":"2026-01-21T06:53:16.329786Z","iopub.status.idle":"2026-01-21T06:53:32.550169Z","shell.execute_reply.started":"2026-01-21T06:53:16.329751Z","shell.execute_reply":"2026-01-21T06:53:32.549667Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m465.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m563.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m611.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m567.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.5/532.5 kB\u001b[0m \u001b[31m558.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m349.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m427.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m571.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m477.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m443.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nray 2.52.1 requires click!=8.3.*,>=7.0, but you have click 8.3.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CELL 1 (REPLACE) — RAM mode phải chạy TRƯỚC MỌI import transformers/tokenizers/peft\nimport os\nfrom pathlib import Path\n\n!df -h /dev/shm\n\nRAM_BASE = Path(\"/dev/shm/kaggle_ram\")\nRAM_BASE.mkdir(parents=True, exist_ok=True)\n\nHF_HOME = RAM_BASE / \"hf\"\nHF_HOME.mkdir(parents=True, exist_ok=True)\n\nos.environ[\"HF_HOME\"] = str(HF_HOME)\nos.environ[\"HF_HUB_CACHE\"] = str(HF_HOME / \"hub\")\nos.environ[\"HF_DATASETS_CACHE\"] = str(HF_HOME / \"datasets\")\nos.environ[\"TRANSFORMERS_CACHE\"] = str(HF_HOME / \"transformers\")\nos.environ[\"TORCH_HOME\"] = str(RAM_BASE / \"torch\")\nos.environ[\"XDG_CACHE_HOME\"] = str(RAM_BASE / \".cache\")\n\n# outputs/logs cũng đẩy vào RAM\nWORKDIR = RAM_BASE / \"working\"\nDATA_DIR = WORKDIR / \"data\"\nTEACH_CACHE_DIR = WORKDIR / \"teacher_outputs\"\nWORKDIR.mkdir(parents=True, exist_ok=True)\nDATA_DIR.mkdir(parents=True, exist_ok=True)\nTEACH_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"HF_HOME =\", os.environ[\"HF_HOME\"])\nprint(\"HF_HUB_CACHE =\", os.environ[\"HF_HUB_CACHE\"])\nprint(\"WORKDIR =\", WORKDIR)\nprint(\"DATA_DIR =\", DATA_DIR)\nprint(\"TEACH_CACHE_DIR =\", TEACH_CACHE_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T06:53:32.551151Z","iopub.execute_input":"2026-01-21T06:53:32.551284Z","iopub.status.idle":"2026-01-21T06:53:32.666829Z","shell.execute_reply.started":"2026-01-21T06:53:32.551269Z","shell.execute_reply":"2026-01-21T06:53:32.666353Z"}},"outputs":[{"name":"stdout","text":"Filesystem      Size  Used Avail Use% Mounted on\nshm             114G     0  114G   0% /dev/shm\nHF_HOME = /dev/shm/kaggle_ram/hf\nHF_HUB_CACHE = /dev/shm/kaggle_ram/hf/hub\nWORKDIR = /dev/shm/kaggle_ram/working\nDATA_DIR = /dev/shm/kaggle_ram/working/data\nTEACH_CACHE_DIR = /dev/shm/kaggle_ram/working/teacher_outputs\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# CELL 3 (REPLACE) — setup PATHS/QWEN32B_PATH nhưng KHÔNG reset WORKDIR/DATA_DIR về /kaggle/working nữa\nimport os, re, json, time, math, random\nfrom glob import glob\n\n# WORKDIR/DATA_DIR đã được set từ CELL 1 (RAM). Không ghi đè lại.\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\ndef find_qwen32b_path():\n    candidates = []\n    for p in glob(\"/kaggle/input/**\", recursive=True):\n        if os.path.isdir(p):\n            low = p.lower()\n            if \"qwen\" in low and (\"32b\" in low or \"32-b\" in low):\n                if os.path.exists(os.path.join(p, \"config.json\")):\n                    candidates.append(p)\n    candidates = sorted(candidates, key=lambda x: len(x))\n    return candidates[0] if candidates else None\n\nQWEN32B_PATH = find_qwen32b_path()\nprint(\"QWEN32B_PATH =\", QWEN32B_PATH)\n\nTEACHERS = {\n    \"open_finance_8b\": \"DragonLLM/Llama-Open-Finance-8B\",\n    \"finance_llama3_8b\": \"instruction-pretrain/finance-Llama3-8B\",\n    \"fingpt_lora_llama3_8b\": \"FinGPT/fingpt-mt_llama3-8b_lora\",\n}\nFINGPT_BASE = \"meta-llama/Meta-Llama-3-8B\"\n\nPATHS = {\n    \"vn_mcocr\": \"/kaggle/input/vietnamese-receipts-mc-ocr-2021\",\n    \"invoice_ocr\": \"/kaggle/input/invoice-ocr\",\n    \"hi_quality_invoice\": \"/kaggle/input/high-quality-invoice-images-for-ocr\",\n    \"gl_xlsx\": \"/kaggle/input/generalledger/Data file for students.xlsx\",\n    \"transactions_csv\": \"/kaggle/input/financial-transactions-dataset/financial_transactions.csv\",\n    \"forecast_csv\": \"/kaggle/input/financial-forecasting-data/simulated_financial_forecasting_data.csv\",\n    \"data_retriever_csv\": \"/kaggle/input/data-retreiver/Data_ret.csv\",\n}\nprint(\"DATA PATHS OK\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T06:53:32.667550Z","iopub.execute_input":"2026-01-21T06:53:32.667695Z","iopub.status.idle":"2026-01-21T06:56:02.108041Z","shell.execute_reply.started":"2026-01-21T06:53:32.667672Z","shell.execute_reply":"2026-01-21T06:56:02.107601Z"}},"outputs":[{"name":"stdout","text":"QWEN32B_PATH = /kaggle/input/qwen-3/transformers/32b/1\nDATA PATHS OK\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from jsonschema import validate\nfrom jsonschema.exceptions import ValidationError\n\n# ===== Schemas =====\nRECEIPT_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"vendor_name\": {\"type\": [\"string\", \"null\"]},\n        \"address\": {\"type\": [\"string\", \"null\"]},\n        \"date\": {\"type\": [\"string\", \"null\"]},            # YYYY-MM-DD preferred\n        \"total_amount\": {\"type\": [\"number\", \"null\"]},\n        \"currency\": {\"type\": [\"string\", \"null\"]},        # \"VND\"\n        \"confidence\": {\"type\": \"number\"},\n        \"flags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n    },\n    \"required\": [\"vendor_name\",\"address\",\"date\",\"total_amount\",\"currency\",\"confidence\",\"flags\"]\n}\n\nINVOICE_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"vendor_name\": {\"type\": [\"string\", \"null\"]},\n        \"invoice_no\": {\"type\": [\"string\", \"null\"]},\n        \"date\": {\"type\": [\"string\", \"null\"]},\n        \"subtotal\": {\"type\": [\"number\", \"null\"]},\n        \"tax\": {\"type\": [\"number\", \"null\"]},\n        \"total\": {\"type\": [\"number\", \"null\"]},\n        \"currency\": {\"type\": [\"string\", \"null\"]},\n        \"confidence\": {\"type\": \"number\"},\n        \"flags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n    },\n    \"required\": [\"vendor_name\",\"invoice_no\",\"date\",\"subtotal\",\"tax\",\"total\",\"currency\",\"confidence\",\"flags\"]\n}\n\nJOURNAL_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"entries\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"account\": {\"type\": \"string\"},\n                    \"debit\": {\"type\": \"number\"},\n                    \"credit\": {\"type\": \"number\"},\n                    \"memo\": {\"type\": [\"string\",\"null\"]}\n                },\n                \"required\": [\"account\",\"debit\",\"credit\",\"memo\"]\n            }\n        },\n        \"confidence\": {\"type\": \"number\"},\n        \"flags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n    },\n    \"required\": [\"entries\",\"confidence\",\"flags\"]\n}\n\nTASK2SCHEMA = {\n    \"receipt_extract_text\": RECEIPT_SCHEMA,\n    \"invoice_extract_text\": INVOICE_SCHEMA,\n    \"journal_from_structured_txn\": JOURNAL_SCHEMA,\n}\n\ndef schema_pass(task: str, obj: dict) -> bool:\n    try:\n        validate(instance=obj, schema=TASK2SCHEMA[task])\n        return True\n    except ValidationError:\n        return False\n    except Exception:\n        return False\n\n# ===== JSON extract / repair =====\ndef extract_json_from_text(text: str):\n    if text is None:\n        return None\n    # pick first {...} block\n    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n    if not m:\n        return None\n    s = m.group(0)\n    try:\n        return json.loads(s)\n    except Exception:\n        return None\n\ndef json_repair_minimal(text: str):\n    \"\"\"\n    deterministic repair for common LLM issues:\n    - trailing commas\n    - single quotes -> double quotes (simple cases)\n    \"\"\"\n    if text is None:\n        return None\n    m = re.search(r\"\\{.*\\}\", text, flags=re.S)\n    if not m:\n        return None\n    s = m.group(0).strip()\n\n    s = re.sub(r\",\\s*}\", \"}\", s)\n    s = re.sub(r\",\\s*]\", \"]\", s)\n    # naive quote fix (only if it looks like JSON)\n    if \"'\" in s and '\"' not in s:\n        s = s.replace(\"'\", '\"')\n\n    try:\n        return json.loads(s)\n    except Exception:\n        return None\n\n# ===== Prompt builder =====\ndef build_prompt(task: str, input_data):\n    if task == \"receipt_extract_text\":\n        return f\"\"\"\nExtract receipt key fields from Vietnamese text.\nReturn ONLY valid JSON with fields:\nvendor_name,address,date,total_amount,currency,confidence,flags\n\nReceipt Text:\n{input_data}\n\"\"\".strip()\n\n    if task == \"invoice_extract_text\":\n        return f\"\"\"\nExtract invoice fields from text.\nReturn ONLY valid JSON with fields:\nvendor_name,invoice_no,date,subtotal,tax,total,currency,confidence,flags\n\nInvoice Text:\n{input_data}\n\"\"\".strip()\n\n    if task == \"journal_from_structured_txn\":\n        return f\"\"\"\nYou are an ERP accountant.\nGiven a structured transaction JSON, propose journal entries.\nReturn ONLY valid JSON with fields:\n- entries: array of objects (account, debit, credit, memo)\n- confidence\n- flags\n\nTransaction:\n{json.dumps(input_data, ensure_ascii=False)}\n\"\"\".strip()\n\n    raise ValueError(\"Unknown task\")\n\nprint(\"Schemas + prompt builder ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T06:56:02.109323Z","iopub.execute_input":"2026-01-21T06:56:02.109492Z","iopub.status.idle":"2026-01-21T06:56:02.974339Z","shell.execute_reply.started":"2026-01-21T06:56:02.109477Z","shell.execute_reply":"2026-01-21T06:56:02.973906Z"}},"outputs":[{"name":"stdout","text":"Schemas + prompt builder ready.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\n\ndef infer_col(df, candidates):\n    cols = {c.lower(): c for c in df.columns}\n    for cand in candidates:\n        if cand.lower() in cols:\n            return cols[cand.lower()]\n    # fuzzy contains\n    for c in df.columns:\n        low = c.lower()\n        for cand in candidates:\n            if cand.lower() in low:\n                return c\n    return None\n\ndef load_vn_mcocr_cases(limit=300):\n    root = PATHS[\"vn_mcocr\"]\n    cases = []\n\n    # Prefer CSV with gold labels if present\n    csv_candidates = [\n        os.path.join(root, \"mcocr_train_df.csv\"),\n        os.path.join(root, \"mcocr_val_sample_df.csv\"),\n        os.path.join(root, \"results.csv\"),\n    ]\n    for p in csv_candidates:\n        if os.path.exists(p):\n            df = pd.read_csv(p)\n\n            text_col = infer_col(df, [\"text\", \"ocr_text\", \"raw_text\", \"content\", \"transcription\"])\n            if text_col is None:\n                # fallback pick longest string col\n                str_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n                if str_cols:\n                    text_col = max(str_cols, key=lambda c: df[c].astype(str).str.len().mean())\n\n            seller_col = infer_col(df, [\"seller\", \"vendor\", \"vendor_name\", \"merchant\", \"store\", \"shop\"])\n            addr_col   = infer_col(df, [\"address\", \"seller_address\", \"vendor_address\"])\n            date_col   = infer_col(df, [\"timestamp\", \"date\", \"datetime\", \"time\"])\n            total_col  = infer_col(df, [\"total_cost\", \"total\", \"amount\", \"total_amount\", \"sum\"])\n\n            for i, row in df.head(limit).iterrows():\n                raw_text = str(row[text_col]) if text_col else \"\"\n\n                gold = None\n                if seller_col or addr_col or date_col or total_col:\n                    def safe_float(x):\n                        try:\n                            if pd.isna(x): \n                                return None\n                            s = str(x)\n                            s = re.sub(r\"[^\\d\\.\\-]\", \"\", s)\n                            return float(s) if s else None\n                        except:\n                            return None\n\n                    gold = {\n                        \"vendor_name\": str(row[seller_col]) if seller_col and pd.notna(row[seller_col]) else None,\n                        \"address\": str(row[addr_col]) if addr_col and pd.notna(row[addr_col]) else None,\n                        \"date\": str(row[date_col]) if date_col and pd.notna(row[date_col]) else None,\n                        \"total_amount\": safe_float(row[total_col]) if total_col else None,\n                        \"currency\": \"VND\",\n                        \"confidence\": 0.0,\n                        \"flags\": []\n                    }\n\n                cases.append({\n                    \"id\": f\"vn_mcocr_{i}\",\n                    \"task\": \"receipt_extract_text\",\n                    \"input\": raw_text,\n                    \"gold\": gold,\n                    \"meta\": {\"source\": os.path.basename(p)}\n                })\n            return cases\n\n    # fallback txt (OCR lines)\n    txt_candidates = [\n        os.path.join(root, \"text_recognition_train_data.txt\"),\n        os.path.join(root, \"text_recognition_val_data.txt\"),\n    ]\n    for p in txt_candidates:\n        if os.path.exists(p):\n            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                for idx, line in enumerate(f):\n                    if idx >= limit:\n                        break\n                    parts = line.strip().split(\"\\t\")\n                    raw_text = parts[-1] if parts else \"\"\n                    cases.append({\n                        \"id\": f\"vn_mcocr_txt_{idx}\",\n                        \"task\": \"receipt_extract_text\",\n                        \"input\": raw_text,\n                        \"gold\": None,\n                        \"meta\": {\"source\": os.path.basename(p)}\n                    })\n            return cases\n\n    return []\n\ndef load_gl_cases(limit=200):\n    xlsx_path = PATHS[\"gl_xlsx\"]\n    if not os.path.exists(xlsx_path):\n        return []\n    xls = pd.ExcelFile(xlsx_path)\n    # take first sheet by default\n    df = pd.read_excel(xlsx_path, sheet_name=xls.sheet_names[0])\n\n    cases = []\n    for i, row in df.head(limit).iterrows():\n        txn = row.to_dict()\n        cases.append({\n            \"id\": f\"gl_{i}\",\n            \"task\": \"journal_from_structured_txn\",\n            \"input\": txn,\n            \"gold\": None,\n            \"meta\": {\"sheet\": xls.sheet_names[0]}\n        })\n    return cases\n\ndef load_invoice_ocr_cases(limit=200):\n    \"\"\"\n    Robust loader:\n    - If JSON/CSV annotations exist -> use their text fields\n    - Otherwise use image paths (text-only LLM can't read images, but still valid for KD if you later OCR)\n    \"\"\"\n    root = PATHS[\"invoice_ocr\"]\n    if not os.path.exists(root):\n        return []\n\n    ann_files = []\n    for ext in [\"*.json\",\"*.csv\"]:\n        ann_files += glob(os.path.join(root, \"**\", ext), recursive=True)\n\n    cases = []\n    if ann_files:\n        # take first annotation file found\n        p = ann_files[0]\n        if p.endswith(\".csv\"):\n            df = pd.read_csv(p)\n            text_col = infer_col(df, [\"text\",\"ocr\",\"raw\",\"content\"])\n            for i, row in df.head(limit).iterrows():\n                raw_text = str(row[text_col]) if text_col else \"\"\n                cases.append({\n                    \"id\": f\"invoice_ocr_csv_{i}\",\n                    \"task\": \"invoice_extract_text\",\n                    \"input\": raw_text,\n                    \"gold\": None,\n                    \"meta\": {\"ann\": os.path.basename(p)}\n                })\n        else:\n            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                js = json.load(f)\n            # try to find list items with \"text\"\n            items = []\n            if isinstance(js, list):\n                items = js\n            elif isinstance(js, dict):\n                # common keys\n                for k in [\"data\",\"items\",\"annotations\",\"samples\"]:\n                    if k in js and isinstance(js[k], list):\n                        items = js[k]\n                        break\n\n            for i, it in enumerate(items[:limit]):\n                raw_text = it.get(\"text\") or it.get(\"ocr_text\") or it.get(\"content\") or \"\"\n                cases.append({\n                    \"id\": f\"invoice_ocr_json_{i}\",\n                    \"task\": \"invoice_extract_text\",\n                    \"input\": str(raw_text),\n                    \"gold\": None,\n                    \"meta\": {\"ann\": os.path.basename(p)}\n                })\n\n        return cases\n\n    # fallback: use image paths (for later OCR pipeline)\n    imgs = glob(os.path.join(root, \"**\", \"*.png\"), recursive=True) + glob(os.path.join(root, \"**\", \"*.jpg\"), recursive=True)\n    for i, ip in enumerate(imgs[:limit]):\n        cases.append({\n            \"id\": f\"invoice_ocr_img_{i}\",\n            \"task\": \"invoice_extract_text\",\n            \"input\": f\"[IMAGE_PATH]{ip}\",\n            \"gold\": None,\n            \"meta\": {\"img\": os.path.basename(ip)}\n        })\n    return cases\n\nprint(\"Loaders ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T06:56:02.975001Z","iopub.execute_input":"2026-01-21T06:56:02.975342Z","iopub.status.idle":"2026-01-21T06:56:03.237212Z","shell.execute_reply.started":"2026-01-21T06:56:02.975327Z","shell.execute_reply":"2026-01-21T06:56:03.236777Z"}},"outputs":[{"name":"stdout","text":"Loaders ready.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datetime import datetime, date\n\ndef _json_default(o):\n    if isinstance(o, (pd.Timestamp, datetime, date)):\n        return o.isoformat()\n    if isinstance(o, (np.integer,)):\n        return int(o)\n    if isinstance(o, (np.floating,)):\n        return float(o)\n    if isinstance(o, (np.ndarray,)):\n        return o.tolist()\n    return str(o)\n\ndef write_jsonl(path, rows):\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        for r in rows:\n            f.write(json.dumps(r, ensure_ascii=False, default=_json_default) + \"\\n\")\n\n\neval_cases = []\neval_cases += load_vn_mcocr_cases(limit=300)\neval_cases += load_invoice_ocr_cases(limit=200)\neval_cases += load_gl_cases(limit=150)\n\nprint(\"Total eval cases:\", len(eval_cases))\neval_path = str(DATA_DIR / \"eval_cases.jsonl\")\nwrite_jsonl(eval_path, eval_cases)\nprint(\"Saved:\", eval_path)\n\n# KD training uses the same pool (you can enlarge later)\nkd_pool = eval_cases.copy()\nrandom.shuffle(kd_pool)\nkd_pool = kd_pool[:500]  # keep KD small for iteration speed\nkd_pool_path = str(DATA_DIR / \"kd_pool.jsonl\")\nwrite_jsonl(kd_pool_path, kd_pool)\nprint(\"Saved KD pool:\", kd_pool_path, \"| size:\", len(kd_pool))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T06:56:03.237904Z","iopub.execute_input":"2026-01-21T06:56:03.238358Z","iopub.status.idle":"2026-01-21T06:56:04.767933Z","shell.execute_reply.started":"2026-01-21T06:56:03.238340Z","shell.execute_reply":"2026-01-21T06:56:04.767458Z"}},"outputs":[{"name":"stdout","text":"Total eval cases: 450\nSaved: /dev/shm/kaggle_ram/working/data/eval_cases.jsonl\nSaved KD pool: /dev/shm/kaggle_ram/working/data/kd_pool.jsonl | size: 450\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# CELL 4 (REPLACE) — model utils: decode ONLY generated tokens (stop prompt-echo killing JSON parsing)\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\nbnb4 = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\nHF_CACHE_DIR = os.environ[\"HF_HUB_CACHE\"]\n\ndef load_4bit_model(repo_or_path: str):\n    tok = AutoTokenizer.from_pretrained(\n        repo_or_path,\n        use_fast=True,\n        trust_remote_code=True,\n        cache_dir=HF_CACHE_DIR,\n    )\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n    tok.padding_side = \"left\"\n\n    mdl = AutoModelForCausalLM.from_pretrained(\n        repo_or_path,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16,\n        quantization_config=bnb4,\n        trust_remote_code=True,\n        attn_implementation=\"sdpa\",\n        cache_dir=HF_CACHE_DIR,\n    )\n    mdl.eval()\n    return tok, mdl\n\ndef load_fingpt(adapter_repo: str, base_repo: str):\n    tok, base = load_4bit_model(base_repo)\n    mdl = PeftModel.from_pretrained(base, adapter_repo)\n    mdl.eval()\n    return tok, mdl\n\n@torch.no_grad()\ndef generate_batch(tok, mdl, prompts, max_new_tokens=320):\n    enc = tok(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(mdl.device)\n    out = mdl.generate(\n        **enc,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        top_p=1.0,\n        repetition_penalty=1.05,\n        pad_token_id=tok.eos_token_id,\n    )\n    # ✅ decode only the newly generated tokens (avoid full prompt echo)\n    in_len = enc[\"input_ids\"].shape[1]\n    gen = out[:, in_len:]\n    return tok.batch_decode(gen, skip_special_tokens=True)\n\nprint(\"Model utils ready. HF_CACHE_DIR =\", HF_CACHE_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T06:56:04.768677Z","iopub.execute_input":"2026-01-21T06:56:04.769131Z","iopub.status.idle":"2026-01-21T06:56:24.200975Z","shell.execute_reply.started":"2026-01-21T06:56:04.769113Z","shell.execute_reply":"2026-01-21T06:56:24.200480Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n2026-01-21 06:56:13.404357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768978573.543674     106 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768978573.585810     106 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768978573.923219     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768978573.923244     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768978573.923247     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768978573.923248     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Model utils ready. HF_CACHE_DIR = /dev/shm/kaggle_ram/hf/hub\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# CELL 5 (REPLACE) — teacher cell: không hardcode token + TEACH_CACHE_DIR dùng RAM\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nsecret_value_1 = user_secrets.get_secret(\"HF_TOKEN\")\n\nHF_TOKEN = os.getenv(\"HF_TOKEN\", et_value_1)\nif HF_TOKEN:\n    login(token=HF_TOKEN)\n    print(\"HF login OK\")\nelse:\n    print(\"HF_TOKEN missing -> gated teachers may fail\")\n\nTEACH_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n\ndef run_teacher(name, repo=None, adapter=None, base=None, batch_size=16):\n    print(f\"\\n=== Teacher: {name} ===\")\n    try:\n        if adapter and base:\n            tok, mdl = load_fingpt(adapter, base)\n        else:\n            tok, mdl = load_4bit_model(repo)\n\n        with open(kd_pool_path, \"r\", encoding=\"utf-8\") as f:\n            pool = [json.loads(x) for x in f]\n\n        out_path = TEACH_CACHE_DIR / f\"{name}.jsonl\"\n        with open(out_path, \"w\", encoding=\"utf-8\") as fw:\n            for i in range(0, len(pool), batch_size):\n                batch = pool[i:i+batch_size]\n                prompts = [build_prompt(ex[\"task\"], ex[\"input\"]) for ex in batch]\n                texts = generate_batch(tok, mdl, prompts)\n\n                for ex, t in zip(batch, texts):\n                    fw.write(json.dumps({\"id\": ex[\"id\"], \"task\": ex[\"task\"], \"raw\": t}, ensure_ascii=False) + \"\\n\")\n\n        del mdl\n        torch.cuda.empty_cache()\n        print(\"Saved:\", str(out_path))\n        return str(out_path)\n    except Exception as e:\n        print(\"FAILED:\", name, \"| reason:\", str(e))\n        return None\n\n\nteacher_paths = {}\n\n# finance llama3 (public)\nteacher_paths[\"finance_llama3_8b\"] = run_teacher(\"finance_llama3_8b\", repo=TEACHERS[\"finance_llama3_8b\"])\n\n# open finance (gated)\nteacher_paths[\"open_finance_8b\"] = run_teacher(\"open_finance_8b\", repo=TEACHERS[\"open_finance_8b\"])\n\n# fingpt lora (needs base llama3 gated)\nteacher_paths[\"fingpt_lora_llama3_8b\"] = run_teacher(\n    \"fingpt_lora_llama3_8b\",\n    adapter=TEACHERS[\"fingpt_lora_llama3_8b\"],\n    base=FINGPT_BASE\n)\n\nprint(\"\\nTeacher paths:\", teacher_paths)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T06:56:24.201644Z","iopub.execute_input":"2026-01-21T06:56:24.202065Z","iopub.status.idle":"2026-01-21T06:56:24.363845Z","shell.execute_reply.started":"2026-01-21T06:56:24.202048Z","shell.execute_reply":"2026-01-21T06:56:24.363148Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_106/4129139701.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msecret_value_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_secrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mHF_TOKEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0met_value_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mHF_TOKEN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHF_TOKEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'et_value_1' is not defined"],"ename":"NameError","evalue":"name 'et_value_1' is not defined","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"# CELL DISTILL (REPLACE) — robust JSON extraction + coercion so KD distilled != 0\nimport re, json\nfrom datetime import datetime\n\ndef _strip_code_fences(s: str) -> str:\n    if not s:\n        return s\n    s = s.strip()\n    # remove ```json ... ``` or ``` ... ```\n    s = re.sub(r\"^```(?:json)?\\s*\", \"\", s, flags=re.I)\n    s = re.sub(r\"\\s*```$\", \"\", s)\n    return s.strip()\n\ndef _try_json_load(s: str):\n    try:\n        return json.loads(s)\n    except Exception:\n        return None\n\ndef _json_repair_minimal(s: str):\n    if s is None:\n        return None\n    s = _strip_code_fences(s)\n    s = s.strip()\n\n    # remove trailing commas\n    s = re.sub(r\",\\s*}\", \"}\", s)\n    s = re.sub(r\",\\s*]\", \"]\", s)\n\n    # if single quotes and no double quotes (naive)\n    if \"'\" in s and '\"' not in s:\n        s = s.replace(\"'\", '\"')\n\n    return _try_json_load(s)\n\ndef extract_json_robust(text: str):\n    \"\"\"\n    Strategy:\n    1) try whole text (after stripping fences)\n    2) try JSON code block\n    3) scan all {...} candidates and pick the first that parses\n    \"\"\"\n    if text is None:\n        return None\n\n    t = _strip_code_fences(text)\n\n    # 1) whole\n    obj = _try_json_load(t)\n    if obj is not None:\n        return obj\n\n    # 2) try inside ```json ... ```\n    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", text, flags=re.S | re.I)\n    if m:\n        obj = _try_json_load(m.group(1))\n        if obj is not None:\n            return obj\n        obj = _json_repair_minimal(m.group(1))\n        if obj is not None:\n            return obj\n\n    # 3) scan brace blocks non-greedy\n    for m in re.finditer(r\"\\{.*?\\}\", t, flags=re.S):\n        cand = m.group(0)\n        obj = _try_json_load(cand)\n        if obj is not None:\n            return obj\n        obj = _json_repair_minimal(cand)\n        if obj is not None:\n            return obj\n\n    return None\n\ndef _to_float(x):\n    if x is None:\n        return None\n    if isinstance(x, (int, float)):\n        return float(x)\n    s = str(x)\n    s = s.replace(\",\", \".\")\n    s = re.sub(r\"[^0-9\\.\\-]\", \"\", s)\n    try:\n        return float(s) if s else None\n    except Exception:\n        return None\n\ndef _to_str_or_none(x):\n    if x is None:\n        return None\n    s = str(x).strip()\n    return s if s else None\n\ndef _to_flags(x):\n    if x is None:\n        return []\n    if isinstance(x, list):\n        return [str(v) for v in x if str(v).strip()]\n    # split by comma/newline\n    s = str(x)\n    parts = re.split(r\"[,;\\n]+\", s)\n    return [p.strip() for p in parts if p.strip()]\n\ndef coerce_to_schema(task: str, obj: dict):\n    if not isinstance(obj, dict):\n        return None\n\n    if task == \"receipt_extract_text\":\n        out = {\n            \"vendor_name\": _to_str_or_none(obj.get(\"vendor_name\") or obj.get(\"vendor\") or obj.get(\"seller\")),\n            \"address\": _to_str_or_none(obj.get(\"address\") or obj.get(\"addr\")),\n            \"date\": _to_str_or_none(obj.get(\"date\")),\n            \"total_amount\": _to_float(obj.get(\"total_amount\") or obj.get(\"total\") or obj.get(\"amount\")),\n            \"currency\": _to_str_or_none(obj.get(\"currency\")) or \"VND\",\n            \"confidence\": _to_float(obj.get(\"confidence\")) if obj.get(\"confidence\") is not None else 0.5,\n            \"flags\": _to_flags(obj.get(\"flags\")),\n        }\n        return out\n\n    if task == \"invoice_extract_text\":\n        out = {\n            \"vendor_name\": _to_str_or_none(obj.get(\"vendor_name\") or obj.get(\"vendor\") or obj.get(\"seller\")),\n            \"invoice_no\": _to_str_or_none(obj.get(\"invoice_no\") or obj.get(\"invoice_number\") or obj.get(\"invoice\")),\n            \"date\": _to_str_or_none(obj.get(\"date\")),\n            \"subtotal\": _to_float(obj.get(\"subtotal\")),\n            \"tax\": _to_float(obj.get(\"tax\") or obj.get(\"vat\")),\n            \"total\": _to_float(obj.get(\"total\") or obj.get(\"total_amount\") or obj.get(\"amount\")),\n            \"currency\": _to_str_or_none(obj.get(\"currency\")) or \"VND\",\n            \"confidence\": _to_float(obj.get(\"confidence\")) if obj.get(\"confidence\") is not None else 0.5,\n            \"flags\": _to_flags(obj.get(\"flags\")),\n        }\n        return out\n\n    if task == \"journal_from_structured_txn\":\n        entries = obj.get(\"entries\")\n        if not isinstance(entries, list):\n            entries = []\n        norm_entries = []\n        for e in entries:\n            if not isinstance(e, dict):\n                continue\n            norm_entries.append({\n                \"account\": _to_str_or_none(e.get(\"account\")) or \"\",\n                \"debit\": _to_float(e.get(\"debit\")) or 0.0,\n                \"credit\": _to_float(e.get(\"credit\")) or 0.0,\n                \"memo\": _to_str_or_none(e.get(\"memo\")),\n            })\n        out = {\n            \"entries\": norm_entries,\n            \"confidence\": _to_float(obj.get(\"confidence\")) if obj.get(\"confidence\") is not None else 0.5,\n            \"flags\": _to_flags(obj.get(\"flags\")),\n        }\n        return out\n\n    return obj\n\n# ---- run distill again ----\ndef load_teacher_outputs(path):\n    out = {}\n    if not path:\n        return out\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            r = json.loads(line)\n            out[r[\"id\"]] = r\n    return out\n\nteacher_outputs = {k: load_teacher_outputs(v) for k, v in teacher_paths.items() if v}\n\ndef router_priority(task):\n    if task in [\"receipt_extract_text\", \"invoice_extract_text\"]:\n        return [\"open_finance_8b\", \"finance_llama3_8b\", \"fingpt_lora_llama3_8b\"]\n    if task == \"journal_from_structured_txn\":\n        return [\"finance_llama3_8b\", \"open_finance_8b\", \"fingpt_lora_llama3_8b\"]\n    return [\"finance_llama3_8b\", \"open_finance_8b\", \"fingpt_lora_llama3_8b\"]\n\ndistilled = []\ndropped = 0\npicked = {}\n\nwith open(kd_pool_path, \"r\", encoding=\"utf-8\") as f:\n    pool = [json.loads(x) for x in f]\n\nfor ex in pool:\n    task = ex[\"task\"]\n    cid = ex[\"id\"]\n\n    chosen_obj = None\n    chosen_teacher = None\n\n    for tname in router_priority(task):\n        if tname not in teacher_outputs:\n            continue\n        rec = teacher_outputs[tname].get(cid)\n        if not rec:\n            continue\n\n        raw = rec.get(\"raw\", \"\")\n        obj = extract_json_robust(raw)\n        obj = coerce_to_schema(task, obj) if obj is not None else None\n\n        if obj is not None and schema_pass(task, obj):\n            chosen_obj = obj\n            chosen_teacher = tname\n            break\n\n    if chosen_obj is None:\n        dropped += 1\n        continue\n\n    picked[chosen_teacher] = picked.get(chosen_teacher, 0) + 1\n    distilled.append({\n        \"id\": cid,\n        \"task\": task,\n        \"prompt\": build_prompt(task, ex[\"input\"]),\n        \"answer_json\": chosen_obj\n    })\n\nprint(\"KD distilled:\", len(distilled), \"| dropped:\", dropped)\nprint(\"picked_by_teacher:\", picked)\n\ndistill_path = str(DATA_DIR / \"distilled_train.jsonl\")\nwith open(distill_path, \"w\", encoding=\"utf-8\") as f:\n    for r in distilled:\n        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n\nprint(\"Saved:\", distill_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T06:56:24.364297Z","iopub.status.idle":"2026-01-21T06:56:24.364473Z","shell.execute_reply.started":"2026-01-21T06:56:24.364389Z","shell.execute_reply":"2026-01-21T06:56:24.364401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL TRAIN (REPLACE) — fix TRL SFTTrainer API (no tokenizer=)\nfrom datasets import Dataset\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nimport json\n\nassert QWEN32B_PATH is not None, \"Cannot find Qwen3-32B in /kaggle/input\"\n\n# Load Qwen base\nqwen_tok, qwen_base = load_4bit_model(QWEN32B_PATH)\n\ndef guess_lora_targets(model):\n    names = set()\n    for n, _ in model.named_modules():\n        if any(k in n for k in [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]):\n            names.add(n.split(\".\")[-1])\n    return sorted(list(names)) if names else [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n\ntargets = guess_lora_targets(qwen_base)\nprint(\"LoRA targets:\", targets)\n\nlora_cfg = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=targets,\n)\n\nstudent = get_peft_model(qwen_base, lora_cfg)\nstudent.print_trainable_parameters()\n\n# Prepare dataset (guard empty distilled file)\nrows = []\nwith open(distill_path, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        r = json.loads(line)\n        rows.append({\"text\": r[\"prompt\"] + \"\\n\\n\" + json.dumps(r[\"answer_json\"], ensure_ascii=False)})\n\nif len(rows) == 0:\n    raise RuntimeError(\"distilled_train.jsonl is empty (KD distilled = 0). Fix teacher outputs / schema first.\")\n\ntrain_ds = Dataset.from_list(rows)\n\nargs = TrainingArguments(\n    output_dir=str(WORKDIR / \"student_ckpt\"),\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    learning_rate=2e-4,\n    num_train_epochs=1,\n    logging_steps=10,\n    save_steps=200,\n    bf16=True,\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",\n)\n\ntrainer = SFTTrainer(\n    model=student,\n    args=args,\n    train_dataset=train_ds,\n    max_seq_length=2048,\n    processing_class=qwen_tok,   # ✅ thay cho tokenizer=\n)\n\ntrainer.train()\n\nADAPTER_DIR = str(WORKDIR / \"student_adapter\")\ntrainer.model.save_pretrained(ADAPTER_DIR)\nqwen_tok.save_pretrained(ADAPTER_DIR)\n\nprint(\"Saved student adapter:\", ADAPTER_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T06:56:24.365578Z","iopub.status.idle":"2026-01-21T06:56:24.365753Z","shell.execute_reply.started":"2026-01-21T06:56:24.365675Z","shell.execute_reply":"2026-01-21T06:56:24.365685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize_obj(obj):\n    if obj is None:\n        return None\n    # stable dump for determinism comparison\n    return json.dumps(obj, ensure_ascii=False, sort_keys=True)\n\ndef field_exact(gold, pred, key):\n    if gold is None or pred is None:\n        return None\n    if gold.get(key) is None:\n        return None\n    return 1.0 if str(gold.get(key)).strip() == str(pred.get(key)).strip() else 0.0\n\ndef eval_model(tok, mdl, cases, repeats=3, batch_size=8):\n    stats = {\n        \"json_valid_rate\": 0,\n        \"schema_pass_rate\": 0,\n        \"determinism_rate\": 0,\n        \"n\": len(cases),\n        \"field_vendor_acc\": [],\n        \"field_total_acc\": [],\n        \"field_date_acc\": [],\n    }\n\n    det_same = 0\n    valid = 0\n    schema_ok = 0\n\n    for i in range(0, len(cases), batch_size):\n        batch = cases[i:i+batch_size]\n        prompts = [build_prompt(ex[\"task\"], ex[\"input\"]) for ex in batch]\n\n        # determinism: run repeats times\n        outputs_all = []\n        for _ in range(repeats):\n            texts = generate_batch(tok, mdl, prompts, max_new_tokens=320)\n            objs = []\n            for ex, t in zip(batch, texts):\n                obj = extract_json_from_text(t) or json_repair_minimal(t)\n                objs.append(obj)\n            outputs_all.append(objs)\n\n        # compute per-sample stats\n        for j, ex in enumerate(batch):\n            task = ex[\"task\"]\n            gold = ex.get(\"gold\")\n\n            # use first run as \"pred\"\n            pred = outputs_all[0][j]\n\n            if pred is not None:\n                valid += 1\n                if schema_pass(task, pred):\n                    schema_ok += 1\n\n            # determinism check: all normalized equal\n            norms = [normalize_obj(outputs_all[r][j]) for r in range(repeats)]\n            if len(set(norms)) == 1:\n                det_same += 1\n\n            # Tier B field acc if gold exists & relevant\n            if gold and isinstance(gold, dict) and task in [\"receipt_extract_text\",\"invoice_extract_text\"]:\n                v = field_exact(gold, pred, \"vendor_name\")\n                d = field_exact(gold, pred, \"date\")\n                # receipt: total_amount ; invoice: total\n                if task == \"receipt_extract_text\":\n                    tacc = field_exact(gold, pred, \"total_amount\")\n                else:\n                    tacc = field_exact(gold, pred, \"total\")\n\n                if v is not None: stats[\"field_vendor_acc\"].append(v)\n                if d is not None: stats[\"field_date_acc\"].append(d)\n                if tacc is not None: stats[\"field_total_acc\"].append(tacc)\n\n    n = max(1, stats[\"n\"])\n    stats[\"json_valid_rate\"] = valid / n\n    stats[\"schema_pass_rate\"] = schema_ok / n\n    stats[\"determinism_rate\"] = det_same / n\n\n    def avg(x):\n        return float(sum(x)/len(x)) if x else None\n\n    stats[\"vendor_acc\"] = avg(stats[\"field_vendor_acc\"])\n    stats[\"date_acc\"] = avg(stats[\"field_date_acc\"])\n    stats[\"total_acc\"] = avg(stats[\"field_total_acc\"])\n\n    # cleanup arrays\n    stats.pop(\"field_vendor_acc\", None)\n    stats.pop(\"field_date_acc\", None)\n    stats.pop(\"field_total_acc\", None)\n\n    return stats\n\n# Load eval cases\nwith open(eval_path, \"r\", encoding=\"utf-8\") as f:\n    eval_cases = [json.loads(x) for x in f]\n\n# BASE QWEN\nbase_tok, base_mdl = load_4bit_model(QWEN32B_PATH)\nbase_stats = eval_model(base_tok, base_mdl, eval_cases, repeats=3)\nprint(\"BASE:\", base_stats)\n\n# STUDENT = base + adapter\nfrom peft import PeftModel\nstudent_tok, student_base = load_4bit_model(QWEN32B_PATH)\nstudent_mdl = PeftModel.from_pretrained(student_base, ADAPTER_DIR)\nstudent_mdl.eval()\nstudent_stats = eval_model(student_tok, student_mdl, eval_cases, repeats=3)\nprint(\"STUDENT:\", student_stats)\n\nreport = {\n    \"base_qwen3_32b\": base_stats,\n    \"student_qwen3_32b_adapter\": student_stats,\n}\n\nreport_path = str(DATA_DIR / \"eval_report.json\")\nwith open(report_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(report, f, ensure_ascii=False, indent=2)\n\nprint(\"Saved report:\", report_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-21T06:56:24.366253Z","iopub.status.idle":"2026-01-21T06:56:24.366415Z","shell.execute_reply.started":"2026-01-21T06:56:24.366337Z","shell.execute_reply":"2026-01-21T06:56:24.366347Z"}},"outputs":[],"execution_count":null}]}