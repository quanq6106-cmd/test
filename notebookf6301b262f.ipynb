{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":118448,"databundleVersionId":14559231,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":3424966,"sourceType":"datasetVersion","datasetId":2035671},{"sourceId":3818163,"sourceType":"datasetVersion","datasetId":2274483},{"sourceId":4501486,"sourceType":"datasetVersion","datasetId":2631784},{"sourceId":5258124,"sourceType":"datasetVersion","datasetId":3059801},{"sourceId":9971715,"sourceType":"datasetVersion","datasetId":6134857},{"sourceId":11087772,"sourceType":"datasetVersion","datasetId":4576291},{"sourceId":11497644,"sourceType":"datasetVersion","datasetId":7207743},{"sourceId":11564740,"sourceType":"datasetVersion","datasetId":7251054},{"sourceId":11739593,"sourceType":"datasetVersion","datasetId":5773627},{"sourceId":363168,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":301540,"modelId":322000}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# CELL 1 (REPLACE) — RAM mode (PHẢI CHẠY TRƯỚC MỌI import HF/transformers/peft)\n# Vị trí: TOP notebook\n# ============================================================\nimport os\nfrom pathlib import Path\n\n!df -h /dev/shm\n\nRAM_BASE = Path(\"/dev/shm/kaggle_ram\")\nRAM_BASE.mkdir(parents=True, exist_ok=True)\n\nHF_HOME = RAM_BASE / \"hf\"\nHF_HOME.mkdir(parents=True, exist_ok=True)\n\nos.environ[\"HF_HOME\"] = str(HF_HOME)\nos.environ[\"HF_HUB_CACHE\"] = str(HF_HOME / \"hub\")\nos.environ[\"HF_DATASETS_CACHE\"] = str(HF_HOME / \"datasets\")\nos.environ[\"TRANSFORMERS_CACHE\"] = str(HF_HOME / \"transformers\")\nos.environ[\"TORCH_HOME\"] = str(RAM_BASE / \"torch\")\nos.environ[\"XDG_CACHE_HOME\"] = str(RAM_BASE / \".cache\")\n\nWORKDIR = RAM_BASE / \"working\"\nDATA_DIR = WORKDIR / \"data\"\nTEACH_CACHE_DIR = WORKDIR / \"teacher_outputs\"\nWORKDIR.mkdir(parents=True, exist_ok=True)\nDATA_DIR.mkdir(parents=True, exist_ok=True)\nTEACH_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"HF_HOME =\", os.environ[\"HF_HOME\"])\nprint(\"HF_HUB_CACHE =\", os.environ[\"HF_HUB_CACHE\"])\nprint(\"WORKDIR =\", WORKDIR)\nprint(\"DATA_DIR =\", DATA_DIR)\nprint(\"TEACH_CACHE_DIR =\", TEACH_CACHE_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:48:39.096080Z","iopub.execute_input":"2026-01-22T08:48:39.096285Z","iopub.status.idle":"2026-01-22T08:48:39.216682Z","shell.execute_reply.started":"2026-01-22T08:48:39.096269Z","shell.execute_reply":"2026-01-22T08:48:39.216204Z"}},"outputs":[{"name":"stdout","text":"Filesystem      Size  Used Avail Use% Mounted on\nshm             114G     0  114G   0% /dev/shm\nHF_HOME = /dev/shm/kaggle_ram/hf\nHF_HUB_CACHE = /dev/shm/kaggle_ram/hf/hub\nWORKDIR = /dev/shm/kaggle_ram/working\nDATA_DIR = /dev/shm/kaggle_ram/working/data\nTEACH_CACHE_DIR = /dev/shm/kaggle_ram/working/teacher_outputs\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n# CELL 2 (REPLACE) — install libs (KHÔNG import transformers ở đây)\n# Vị trí: ngay sau CELL 1\n# ============================================================\n!pip -q install -U --no-cache-dir \\\n  \"transformers>=4.51.0\" \\\n  \"accelerate>=0.30.0\" \\\n  \"datasets>=2.19.0\" \\\n  \"peft>=0.11.0\" \\\n  \"trl>=0.9.6\" \\\n  \"huggingface_hub>=0.23.0\" \\\n  \"tokenizers>=0.21.0\" \\\n  \"safetensors>=0.4.3\" \\\n  \"sentencepiece\" \\\n  \"jsonschema>=4.22.0\" \\\n  \"rapidfuzz>=3.9.0\" \\\n  \"openpyxl>=3.1.5\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:48:39.217647Z","iopub.execute_input":"2026-01-22T08:48:39.217791Z","iopub.status.idle":"2026-01-22T08:48:59.070896Z","shell.execute_reply.started":"2026-01-22T08:48:39.217774Z","shell.execute_reply":"2026-01-22T08:48:59.070435Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m215.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m619.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m432.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m558.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.5/532.5 kB\u001b[0m \u001b[31m281.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m260.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m541.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m420.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m244.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nray 2.52.1 requires click!=8.3.*,>=7.0, but you have click 8.3.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================\n# CELL 3 (REPLACE) — setup PATHS/QWEN32B_PATH\n# Vị trí: sau CELL 2\n# ============================================================\nimport os, re, json, random\nfrom glob import glob\n\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\ndef find_qwen32b_path():\n    candidates = []\n    for p in glob(\"/kaggle/input/**\", recursive=True):\n        if os.path.isdir(p):\n            low = p.lower()\n            if \"qwen\" in low and (\"32b\" in low or \"32-b\" in low):\n                if os.path.exists(os.path.join(p, \"config.json\")):\n                    candidates.append(p)\n    candidates = sorted(candidates, key=lambda x: len(x))\n    return candidates[0] if candidates else None\n\nQWEN32B_PATH = find_qwen32b_path()\nprint(\"QWEN32B_PATH =\", QWEN32B_PATH)\n\nTEACHERS = {\n    \"open_finance_8b\": \"DragonLLM/Llama-Open-Finance-8B\",\n    \"finance_llama3_8b\": \"instruction-pretrain/finance-Llama3-8B\",\n    \"fingpt_lora_llama3_8b\": \"FinGPT/fingpt-mt_llama3-8b_lora\",\n}\nFINGPT_BASE = \"meta-llama/Meta-Llama-3-8B\"\n\nPATHS = {\n    \"vn_mcocr\": \"/kaggle/input/vietnamese-receipts-mc-ocr-2021\",\n    \"invoice_ocr\": \"/kaggle/input/invoice-ocr\",\n    \"hi_quality_invoice\": \"/kaggle/input/high-quality-invoice-images-for-ocr\",\n    \"gl_xlsx\": \"/kaggle/input/generalledger/Data file for students.xlsx\",\n    \"transactions_csv\": \"/kaggle/input/financial-transactions-dataset/financial_transactions.csv\",\n    \"forecast_csv\": \"/kaggle/input/financial-forecasting-data/simulated_financial_forecasting_data.csv\",\n    \"data_retriever_csv\": \"/kaggle/input/data-retreiver/Data_ret.csv\",\n}\nprint(\"DATA PATHS OK\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:48:59.071649Z","iopub.execute_input":"2026-01-22T08:48:59.071782Z","iopub.status.idle":"2026-01-22T08:54:26.271998Z","shell.execute_reply.started":"2026-01-22T08:48:59.071766Z","shell.execute_reply":"2026-01-22T08:54:26.271551Z"}},"outputs":[{"name":"stdout","text":"QWEN32B_PATH = /kaggle/input/qwen-3/transformers/32b/1\nDATA PATHS OK\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================\n# CELL 4 (REPLACE) — schemas + schema_pass + build_prompt\n# Vị trí: sau CELL 3\n# ============================================================\nfrom jsonschema import validate\nfrom jsonschema.exceptions import ValidationError\nimport json\n\nRECEIPT_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"vendor_name\": {\"type\": [\"string\", \"null\"]},\n        \"address\": {\"type\": [\"string\", \"null\"]},\n        \"date\": {\"type\": [\"string\", \"null\"]},\n        \"total_amount\": {\"type\": [\"number\", \"null\"]},\n        \"currency\": {\"type\": [\"string\", \"null\"]},\n        \"confidence\": {\"type\": \"number\"},\n        \"flags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n    },\n    \"required\": [\"vendor_name\",\"address\",\"date\",\"total_amount\",\"currency\",\"confidence\",\"flags\"],\n}\n\nINVOICE_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"vendor_name\": {\"type\": [\"string\", \"null\"]},\n        \"invoice_no\": {\"type\": [\"string\", \"null\"]},\n        \"date\": {\"type\": [\"string\", \"null\"]},\n        \"subtotal\": {\"type\": [\"number\", \"null\"]},\n        \"tax\": {\"type\": [\"number\", \"null\"]},\n        \"total\": {\"type\": [\"number\", \"null\"]},\n        \"currency\": {\"type\": [\"string\", \"null\"]},\n        \"confidence\": {\"type\": \"number\"},\n        \"flags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n    },\n    \"required\": [\"vendor_name\",\"invoice_no\",\"date\",\"subtotal\",\"tax\",\"total\",\"currency\",\"confidence\",\"flags\"],\n}\n\nJOURNAL_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"entries\": {\n            \"type\": \"array\",\n            \"items\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"account\": {\"type\": \"string\"},\n                    \"debit\": {\"type\": \"number\"},\n                    \"credit\": {\"type\": \"number\"},\n                    \"memo\": {\"type\": [\"string\",\"null\"]},\n                },\n                \"required\": [\"account\",\"debit\",\"credit\",\"memo\"],\n            },\n        },\n        \"confidence\": {\"type\": \"number\"},\n        \"flags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n    },\n    \"required\": [\"entries\",\"confidence\",\"flags\"],\n}\n\nTASK2SCHEMA = {\n    \"receipt_extract_text\": RECEIPT_SCHEMA,\n    \"invoice_extract_text\": INVOICE_SCHEMA,\n    \"journal_from_structured_txn\": JOURNAL_SCHEMA,\n}\n\ndef schema_pass(task: str, obj: dict) -> bool:\n    try:\n        validate(instance=obj, schema=TASK2SCHEMA[task])\n        return True\n    except ValidationError:\n        return False\n    except Exception:\n        return False\n\ndef build_prompt(task: str, input_data):\n    if task == \"receipt_extract_text\":\n        return f\"\"\"\nExtract receipt key fields from Vietnamese text.\nReturn ONLY valid JSON with fields:\nvendor_name,address,date,total_amount,currency,confidence,flags\n\nReceipt Text:\n{input_data}\n\"\"\".strip()\n\n    if task == \"invoice_extract_text\":\n        return f\"\"\"\nExtract invoice fields from text.\nReturn ONLY valid JSON with fields:\nvendor_name,invoice_no,date,subtotal,tax,total,currency,confidence,flags\n\nInvoice Text:\n{input_data}\n\"\"\".strip()\n\n    if task == \"journal_from_structured_txn\":\n        return f\"\"\"\nYou are an ERP accountant.\nGiven a structured transaction JSON, propose journal entries.\nReturn ONLY valid JSON with fields:\n- entries: array of objects (account, debit, credit, memo)\n- confidence\n- flags\n\nTransaction:\n{json.dumps(input_data, ensure_ascii=False)}\n\"\"\".strip()\n\n    raise ValueError(\"Unknown task\")\n\nprint(\"Schemas + prompt builder ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:54:26.273028Z","iopub.execute_input":"2026-01-22T08:54:26.273176Z","iopub.status.idle":"2026-01-22T08:54:27.155397Z","shell.execute_reply.started":"2026-01-22T08:54:26.273161Z","shell.execute_reply":"2026-01-22T08:54:27.154971Z"}},"outputs":[{"name":"stdout","text":"Schemas + prompt builder ready.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# =======================\n# CELL 4B (REPLACE)\n# Model utils (BF16) + PEFT adapter loader\n# Vị trí: thay CELL model utils cũ (trước teacher cell)\n# =======================\n\nimport os\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\nHF_CACHE_DIR = os.environ.get(\"HF_HUB_CACHE\", None)\n\ndef assert_cuda_bf16_ready():\n    assert torch.cuda.is_available(), \"CUDA is not available. You said H100 runtime.\"\n    name = torch.cuda.get_device_name(0)\n    print(\"CUDA device:\", name)\n    assert torch.cuda.is_bf16_supported(), \"bf16 not supported on this GPU runtime.\"\n    return name\n\ndef _pick_attn_impl(prefer_fa2: bool = True) -> str:\n    \"\"\"\n    flash_attention_2 requires flash_attn installed.\n    If not installed -> fall back to sdpa (always available).\n    \"\"\"\n    if prefer_fa2:\n        try:\n            import flash_attn  # noqa: F401\n            return \"flash_attention_2\"\n        except Exception:\n            return \"sdpa\"\n    return \"sdpa\"\n\ndef load_bf16_model(repo_or_path: str, prefer_fa2: bool = True):\n    \"\"\"\n    Full BF16 weights on GPU. Suitable for H100.\n    \"\"\"\n    assert_cuda_bf16_ready()\n    attn_impl = _pick_attn_impl(prefer_fa2=prefer_fa2)\n    print(\"Attention implementation:\", attn_impl)\n\n    tok = AutoTokenizer.from_pretrained(\n        repo_or_path,\n        use_fast=True,\n        trust_remote_code=True,\n        cache_dir=HF_CACHE_DIR,\n    )\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n    tok.padding_side = \"left\"\n\n    mdl = AutoModelForCausalLM.from_pretrained(\n        repo_or_path,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16,\n        trust_remote_code=True,\n        attn_implementation=attn_impl,\n        cache_dir=HF_CACHE_DIR,\n    )\n    mdl.eval()\n    return tok, mdl\n\ndef load_fingpt(adapter_repo: str, base_repo: str, prefer_fa2: bool = True):\n    \"\"\"\n    Load base BF16 model + attach LoRA adapter (FinGPT style).\n    \"\"\"\n    tok, base = load_bf16_model(base_repo, prefer_fa2=prefer_fa2)\n    mdl = PeftModel.from_pretrained(base, adapter_repo)\n    mdl.eval()\n    return tok, mdl\n\n@torch.no_grad()\ndef generate_batch(tok, mdl, prompts, max_new_tokens=256):\n    enc = tok(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(mdl.device)\n    out = mdl.generate(\n        **enc,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        top_p=1.0,\n        repetition_penalty=1.05,\n        pad_token_id=tok.eos_token_id,\n    )\n    in_len = enc[\"input_ids\"].shape[1]\n    gen = out[:, in_len:]\n    return tok.batch_decode(gen, skip_special_tokens=True)\n\nprint(\"Model utils ready (BF16 + PEFT adapter loader).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:54:27.155973Z","iopub.execute_input":"2026-01-22T08:54:27.156103Z","iopub.status.idle":"2026-01-22T08:55:00.814177Z","shell.execute_reply.started":"2026-01-22T08:54:27.156088Z","shell.execute_reply":"2026-01-22T08:55:00.813703Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n2026-01-22 08:54:44.353036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769072084.798651     106 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769072084.908022     106 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769072085.885254     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769072085.885283     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769072085.885285     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769072085.885287     106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Model utils ready (BF16 + PEFT adapter loader).\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================\n# CELL 6 (REPLACE) — loaders (GIỮ NGUYÊN loader của bạn nhưng FIX gold=0 bằng fallback scan columns)\n# Vị trí: sau CELL 4B\n# ============================================================\nimport pandas as pd\nimport os, re, json\nfrom glob import glob\n\ndef infer_col(df, candidates):\n    cols = {c.lower(): c for c in df.columns}\n    for cand in candidates:\n        if cand.lower() in cols:\n            return cols[cand.lower()]\n    for c in df.columns:\n        low = c.lower()\n        for cand in candidates:\n            if cand.lower() in low:\n                return c\n    return None\n\ndef load_vn_mcocr_cases(limit=2000):\n    root = PATHS[\"vn_mcocr\"]\n    cases = []\n\n    csv_candidates = [\n        os.path.join(root, \"mcocr_train_df.csv\"),\n        os.path.join(root, \"mcocr_val_sample_df.csv\"),\n        os.path.join(root, \"results.csv\"),\n    ]\n\n    for p in csv_candidates:\n        if os.path.exists(p):\n            df = pd.read_csv(p)\n\n            text_col = infer_col(df, [\"text\",\"ocr_text\",\"raw_text\",\"content\",\"transcription\"])\n            if text_col is None:\n                str_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n                if str_cols:\n                    text_col = max(str_cols, key=lambda c: df[c].astype(str).str.len().mean())\n\n            seller_col = infer_col(df, [\"seller\",\"vendor\",\"vendor_name\",\"merchant\",\"store\",\"shop\"])\n            addr_col   = infer_col(df, [\"address\",\"seller_address\",\"vendor_address\"])\n            date_col   = infer_col(df, [\"date\",\"timestamp\",\"datetime\",\"time\"])\n            total_col  = infer_col(df, [\"total_cost\",\"total\",\"amount\",\"total_amount\",\"sum\"])\n\n            def safe_float(x):\n                try:\n                    if pd.isna(x): \n                        return None\n                    s = str(x)\n                    s = re.sub(r\"[^\\d\\.\\-]\", \"\", s)\n                    return float(s) if s else None\n                except:\n                    return None\n\n            for i, row in df.head(limit).iterrows():\n                raw_text = str(row[text_col]) if text_col else \"\"\n\n                gold = None\n                # chỉ tạo gold nếu có ít nhất 1 cột label tồn tại\n                if any([seller_col, addr_col, date_col, total_col]):\n                    gold = {\n                        \"vendor_name\": str(row[seller_col]) if seller_col and pd.notna(row[seller_col]) else None,\n                        \"address\": str(row[addr_col]) if addr_col and pd.notna(row[addr_col]) else None,\n                        \"date\": str(row[date_col]) if date_col and pd.notna(row[date_col]) else None,\n                        \"total_amount\": safe_float(row[total_col]) if total_col else None,\n                        \"currency\": \"VND\",\n                        \"confidence\": 0.0,\n                        \"flags\": [],\n                    }\n\n                cases.append({\n                    \"id\": f\"vn_mcocr_{i}\",\n                    \"task\": \"receipt_extract_text\",\n                    \"input\": raw_text,\n                    \"gold\": gold,\n                    \"meta\": {\"source\": os.path.basename(p), \"cols\": list(df.columns)},\n                })\n            return cases\n\n    return cases\n\ndef load_gl_cases(limit=2000):\n    xlsx_path = PATHS[\"gl_xlsx\"]\n    if not os.path.exists(xlsx_path):\n        return []\n    xls = pd.ExcelFile(xlsx_path)\n    df = pd.read_excel(xlsx_path, sheet_name=xls.sheet_names[0])\n\n    cases = []\n    for i, row in df.head(limit).iterrows():\n        cases.append({\n            \"id\": f\"gl_{i}\",\n            \"task\": \"journal_from_structured_txn\",\n            \"input\": row.to_dict(),\n            \"gold\": None,\n            \"meta\": {\"sheet\": xls.sheet_names[0]},\n        })\n    return cases\n\ndef load_invoice_ocr_cases(limit=2000):\n    root = PATHS[\"invoice_ocr\"]\n    if not os.path.exists(root):\n        return []\n\n    ann_files = []\n    for ext in [\"*.json\",\"*.csv\"]:\n        ann_files += glob(os.path.join(root, \"**\", ext), recursive=True)\n\n    cases = []\n    if ann_files:\n        p = ann_files[0]\n        if p.endswith(\".csv\"):\n            df = pd.read_csv(p)\n            text_col = infer_col(df, [\"text\",\"ocr\",\"raw\",\"content\"])\n            for i, row in df.head(limit).iterrows():\n                raw_text = str(row[text_col]) if text_col else \"\"\n                cases.append({\n                    \"id\": f\"invoice_ocr_csv_{i}\",\n                    \"task\": \"invoice_extract_text\",\n                    \"input\": raw_text,\n                    \"gold\": None,\n                    \"meta\": {\"ann\": os.path.basename(p)},\n                })\n        else:\n            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n                js = json.load(f)\n            items = []\n            if isinstance(js, list):\n                items = js\n            elif isinstance(js, dict):\n                for k in [\"data\",\"items\",\"annotations\",\"samples\"]:\n                    if k in js and isinstance(js[k], list):\n                        items = js[k]\n                        break\n            for i, it in enumerate(items[:limit]):\n                raw_text = it.get(\"text\") or it.get(\"ocr_text\") or it.get(\"content\") or \"\"\n                cases.append({\n                    \"id\": f\"invoice_ocr_json_{i}\",\n                    \"task\": \"invoice_extract_text\",\n                    \"input\": str(raw_text),\n                    \"gold\": None,\n                    \"meta\": {\"ann\": os.path.basename(p)},\n                })\n        return cases\n\n    return cases\n\nprint(\"Loaders ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:55:00.814844Z","iopub.execute_input":"2026-01-22T08:55:00.815228Z","iopub.status.idle":"2026-01-22T08:55:00.827497Z","shell.execute_reply.started":"2026-01-22T08:55:00.815209Z","shell.execute_reply":"2026-01-22T08:55:00.827117Z"}},"outputs":[{"name":"stdout","text":"Loaders ready.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# CELL SPLIT (REPLACE) — tách MCOCR (gold) riêng + tạo kd_pool lớn hơn nhưng có kiểm soát\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, date\nimport random, json, re\nfrom pathlib import Path\n\ndef _json_default(o):\n    if isinstance(o, (pd.Timestamp, datetime, date)):\n        return o.isoformat()\n    if isinstance(o, (np.integer,)):\n        return int(o)\n    if isinstance(o, (np.floating,)):\n        return float(o)\n    if isinstance(o, (np.ndarray,)):\n        return o.tolist()\n    return str(o)\n\ndef write_jsonl(path, rows):\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        for r in rows:\n            f.write(json.dumps(r, ensure_ascii=False, default=_json_default) + \"\\n\")\n\n# 1) Load dữ liệu\nmcocr_cases = load_vn_mcocr_cases(limit=2000)          # tăng để có nhiều gold\ninvoice_cases = load_invoice_ocr_cases(limit=2000)     # tăng, nhưng thường thiếu gold\ngl_cases = load_gl_cases(limit=2000)\n\n# 2) Split MCOCR có gold để EVAL chuẩn\nmcocr_gold = [c for c in mcocr_cases if isinstance(c.get(\"gold\"), dict)]\nrandom.shuffle(mcocr_gold)\n\n# Giữ eval_gold đủ lớn để đo; train_gold còn lại để KD/augment nếu muốn\nN_EVAL_GOLD = min(500, len(mcocr_gold))\neval_gold_cases = mcocr_gold[:N_EVAL_GOLD]\ntrain_gold_cases = mcocr_gold[N_EVAL_GOLD:]\n\n# 3) Eval set: ưu tiên MCOCR gold + thêm journal để đo schema/journal\neval_cases = []\neval_cases += eval_gold_cases\n# thêm journal (không có gold) để đo schema strict\neval_cases += gl_cases[:500]\n\n# (tuỳ chọn) invoice text-only nếu có (nếu input là IMAGE_PATH thì eval text-only LLM sẽ fail => bỏ)\ninvoice_text_only = [c for c in invoice_cases if isinstance(c.get(\"input\"), str) and not str(c[\"input\"]).startswith(\"[IMAGE_PATH]\")]\neval_cases += invoice_text_only[:300]\n\nrandom.shuffle(eval_cases)\n\n# 4) KD pool: lấy (train_gold + invoice_text_only + gl) => lớn, đa dạng\nkd_pool = []\nkd_pool += train_gold_cases\nkd_pool += invoice_text_only\nkd_pool += gl_cases\nrandom.shuffle(kd_pool)\n\n# giới hạn để iteration ổn; bạn có thể nâng dần (2k, 5k, 10k)\nKD_POOL_MAX = min(5000, len(kd_pool))\nkd_pool = kd_pool[:KD_POOL_MAX]\n\nprint(\"MCOCR total:\", len(mcocr_cases), \"| gold:\", len(mcocr_gold))\nprint(\"EVAL cases:\", len(eval_cases), \"| EVAL gold:\", len(eval_gold_cases))\nprint(\"KD pool:\", len(kd_pool))\n\neval_path = str(DATA_DIR / \"eval_cases.jsonl\")\nkd_pool_path = str(DATA_DIR / \"kd_pool.jsonl\")\nwrite_jsonl(eval_path, eval_cases)\nwrite_jsonl(kd_pool_path, kd_pool)\n\nprint(\"Saved eval_path:\", eval_path)\nprint(\"Saved kd_pool_path:\", kd_pool_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:55:00.828041Z","iopub.execute_input":"2026-01-22T08:55:00.828167Z","iopub.status.idle":"2026-01-22T08:55:04.951703Z","shell.execute_reply.started":"2026-01-22T08:55:00.828154Z","shell.execute_reply":"2026-01-22T08:55:04.951251Z"}},"outputs":[{"name":"stdout","text":"MCOCR total: 1155 | gold: 0\nEVAL cases: 500 | EVAL gold: 0\nKD pool: 2000\nSaved eval_path: /dev/shm/kaggle_ram/working/data/eval_cases.jsonl\nSaved kd_pool_path: /dev/shm/kaggle_ram/working/data/kd_pool.jsonl\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# =======================\n# CELL 5 (REPLACE)\n# Teacher generation (BF16) - 3 teachers\n# Vị trí: ngay sau CELL 4B\n# =======================\n\nimport json\nimport torch\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nsecret_value_1 = user_secrets.get_secret(\"HF_TOKEN\")\nHF_TOKEN = os.getenv(\"HF_TOKEN\", secret_value_1)\n\nif HF_TOKEN:\n    login(token=HF_TOKEN)\n    print(\"HF login OK\")\nelse:\n    print(\"HF_TOKEN missing -> gated teachers may fail\")\n\nTEACH_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n\ndef run_teacher(name, repo=None, adapter=None, base=None, batch_size=16, max_new_tokens=256):\n    print(f\"\\n=== Teacher: {name} ===\")\n    try:\n        if adapter and base:\n            tok, mdl = load_fingpt(adapter, base)\n        else:\n            tok, mdl = load_bf16_model(repo)\n\n        with open(kd_pool_path, \"r\", encoding=\"utf-8\") as f:\n            pool = [json.loads(x) for x in f]\n\n        out_path = TEACH_CACHE_DIR / f\"{name}.jsonl\"\n        with open(out_path, \"w\", encoding=\"utf-8\") as fw:\n            for i in range(0, len(pool), batch_size):\n                batch = pool[i:i+batch_size]\n                prompts = [build_prompt(ex[\"task\"], ex[\"input\"]) for ex in batch]\n                texts = generate_batch(tok, mdl, prompts, max_new_tokens=max_new_tokens)\n\n                for ex, t in zip(batch, texts):\n                    fw.write(json.dumps({\"id\": ex[\"id\"], \"task\": ex[\"task\"], \"raw\": t}, ensure_ascii=False) + \"\\n\")\n\n        del mdl\n        torch.cuda.empty_cache()\n        print(\"Saved:\", str(out_path))\n        return str(out_path)\n\n    except Exception as e:\n        print(\"FAILED:\", name, \"| reason:\", str(e))\n        return None\n\nteacher_paths = {}\n\nteacher_paths[\"finance_llama3_8b\"] = run_teacher(\n    \"finance_llama3_8b\",\n    repo=TEACHERS[\"finance_llama3_8b\"],\n)\n\nteacher_paths[\"open_finance_8b\"] = run_teacher(\n    \"open_finance_8b\",\n    repo=TEACHERS[\"open_finance_8b\"],\n)\n\nteacher_paths[\"fingpt_lora_llama3_8b\"] = run_teacher(\n    \"fingpt_lora_llama3_8b\",\n    adapter=TEACHERS[\"fingpt_lora_llama3_8b\"],\n    base=FINGPT_BASE,\n)\n\nprint(\"\\nTeacher paths:\", teacher_paths)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T08:55:04.952366Z","iopub.execute_input":"2026-01-22T08:55:04.952716Z","iopub.status.idle":"2026-01-22T09:19:12.130665Z","shell.execute_reply.started":"2026-01-22T08:55:04.952699Z","shell.execute_reply":"2026-01-22T09:19:12.130209Z"}},"outputs":[{"name":"stdout","text":"HF login OK\n\n=== Teacher: finance_llama3_8b ===\nCUDA device: NVIDIA H100 80GB HBM3\nAttention implementation: sdpa\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"891157c0295d4090b1b879e65b47da35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e983bf2b57f4c3ebe6613a6371f4c7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03bdf8e306644d569bf5e62670d868e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/705 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af90b3ce990f4198af04247217e02183"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c193221960be43ab96416ddf093190ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcb059a48b32412aa001697ce302e1f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00007.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"155c34452ba9471b945eb0c89a7ffa93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"149d880b57594b449ba79ee2f2bb6013"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00007.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bb6fd9626774e29918666da02817b17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16b1902e855342b1b24ce24916d02f15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00007.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eac52c26aa946cf9f3abd54d203ebc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84cbb3f4528f4ade82099d6dd50b2899"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00007.safetensors:   0%|          | 0.00/2.57G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"675e5c7bcc0e4a2892e8d61bb38085ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e01af2027fb47d386bb52aadd6e273f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"368341b354934cbf92aaa2992345462c"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Saved: /dev/shm/kaggle_ram/working/teacher_outputs/finance_llama3_8b.jsonl\n\n=== Teacher: open_finance_8b ===\nCUDA device: NVIDIA H100 80GB HBM3\nAttention implementation: sdpa\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7cfbddb478b407a880aeb529df518bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b58a10a93a043a3b61f2713029f6112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/439 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0a3ac7738ab4361823fc3bf4a9952ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja:   0%|          | 0.00/4.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7953a44e9186474da971efd67706f535"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/860 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af0e42cee9bb4f7280cb6b2d2669cc4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e134221894b4380ab42e624ad9a8b69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4a3c8faf97947b2b7de7690c2dbf2bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16048559980e4c48b3e474cbc42cf656"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00007.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e774b6253bd4ddabb9f808e629467e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caaeab3b7c47429da56e8bbb3e011f44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00007.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f4b64f635c04fbc8ab8f0c082b46910"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00007.safetensors:   0%|          | 0.00/4.83G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23828e4090be4c79be55991300c230e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00007.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5611baa5e85d4e9dbbeeea26a5847120"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00007.safetensors:   0%|          | 0.00/2.57G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f2e8c3add0c48f299e2ba7e938d5887"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2503dc92b47c420e98d6621abcda3473"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8adcc5148774267abcfa92bba117bd0"}},"metadata":{}},{"name":"stdout","text":"Saved: /dev/shm/kaggle_ram/working/teacher_outputs/open_finance_8b.jsonl\n\n=== Teacher: fingpt_lora_llama3_8b ===\nCUDA device: NVIDIA H100 80GB HBM3\nAttention implementation: sdpa\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ce25c99f67148f89850854bbae9067a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95f062c71bf24c0684c46d24e6b6dee3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f5f07d9923e45f496bf11191d8577a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5705ac254cb4c24a2c862905da21ba9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61e2b1034aba4628b22e0fdc59e6b168"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"887c0f8060094fb2b2397fd44fd861b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"698cea600d604cceb2f293f528e47033"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f71e1914609d43838c1f4efc51a2b640"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec985c87522840a3a18ad5b6dd759e0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c96b4694576b4645bc3b1e7ba2791ec9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd30df371bf347c6a348b86976037da6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/177 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cd9470e0fd94cff997d76441660f081"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a10a581354234e4583c0243a58bf0a2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/13.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aed756a44cbe4a0ebca49a1ea3d7f4e4"}},"metadata":{}},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"Saved: /dev/shm/kaggle_ram/working/teacher_outputs/fingpt_lora_llama3_8b.jsonl\n\nTeacher paths: {'finance_llama3_8b': '/dev/shm/kaggle_ram/working/teacher_outputs/finance_llama3_8b.jsonl', 'open_finance_8b': '/dev/shm/kaggle_ram/working/teacher_outputs/open_finance_8b.jsonl', 'fingpt_lora_llama3_8b': '/dev/shm/kaggle_ram/working/teacher_outputs/fingpt_lora_llama3_8b.jsonl'}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================\n# CELL DISTILL (REPLACE) — CLEAN teachers:\n#  - parse OK + schema OK\n#  - nếu receipt có gold => gần đúng gold (vendor fuzzy + amount close + date exact nếu có)\n#  - KHÔNG fill default\n# Vị trí: sau teacher cell\n# ============================================================\nimport re, json\nfrom rapidfuzz import fuzz\n\ndef _strip_code_fences(s: str) -> str:\n    if not s:\n        return s\n    s = s.strip()\n    s = re.sub(r\"^```(?:json)?\\s*\", \"\", s, flags=re.I)\n    s = re.sub(r\"\\s*```$\", \"\", s)\n    return s.strip()\n\ndef _try_json_load(s: str):\n    try:\n        return json.loads(s)\n    except Exception:\n        return None\n\ndef extract_json_robust(text: str):\n    if text is None:\n        return None\n    t = _strip_code_fences(text)\n\n    obj = _try_json_load(t)\n    if obj is not None:\n        return obj\n\n    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", text, flags=re.S | re.I)\n    if m:\n        obj = _try_json_load(m.group(1))\n        if obj is not None:\n            return obj\n\n    for m in re.finditer(r\"\\{.*?\\}\", t, flags=re.S):\n        obj = _try_json_load(m.group(0))\n        if obj is not None:\n            return obj\n    return None\n\ndef _to_float(x):\n    if x is None:\n        return None\n    if isinstance(x, (int, float)):\n        return float(x)\n    s = str(x).replace(\",\", \".\")\n    s = re.sub(r\"[^0-9\\.\\-]\", \"\", s)\n    try:\n        return float(s) if s else None\n    except Exception:\n        return None\n\ndef _to_str(x):\n    if x is None:\n        return None\n    s = str(x).strip()\n    return s if s else None\n\ndef _to_flags(x):\n    if x is None:\n        return None\n    if isinstance(x, list):\n        return [str(v).strip() for v in x if str(v).strip()]\n    return [p.strip() for p in re.split(r\"[,;\\n]+\", str(x)) if p.strip()]\n\ndef coerce_strict(task: str, obj: dict):\n    if not isinstance(obj, dict):\n        return None\n    if task == \"receipt_extract_text\":\n        return {\n            \"vendor_name\": _to_str(obj.get(\"vendor_name\") or obj.get(\"vendor\") or obj.get(\"seller\")),\n            \"address\": _to_str(obj.get(\"address\") or obj.get(\"addr\")),\n            \"date\": _to_str(obj.get(\"date\")),\n            \"total_amount\": _to_float(obj.get(\"total_amount\") or obj.get(\"total\") or obj.get(\"amount\")),\n            \"currency\": _to_str(obj.get(\"currency\")),\n            \"confidence\": _to_float(obj.get(\"confidence\")),\n            \"flags\": _to_flags(obj.get(\"flags\")),\n        }\n    if task == \"invoice_extract_text\":\n        return {\n            \"vendor_name\": _to_str(obj.get(\"vendor_name\") or obj.get(\"vendor\") or obj.get(\"seller\")),\n            \"invoice_no\": _to_str(obj.get(\"invoice_no\") or obj.get(\"invoice_number\") or obj.get(\"invoice\")),\n            \"date\": _to_str(obj.get(\"date\")),\n            \"subtotal\": _to_float(obj.get(\"subtotal\")),\n            \"tax\": _to_float(obj.get(\"tax\") or obj.get(\"vat\")),\n            \"total\": _to_float(obj.get(\"total\") or obj.get(\"total_amount\") or obj.get(\"amount\")),\n            \"currency\": _to_str(obj.get(\"currency\")),\n            \"confidence\": _to_float(obj.get(\"confidence\")),\n            \"flags\": _to_flags(obj.get(\"flags\")),\n        }\n    if task == \"journal_from_structured_txn\":\n        entries = obj.get(\"entries\")\n        if not isinstance(entries, list):\n            entries = []\n        norm = []\n        for e in entries:\n            if not isinstance(e, dict):\n                continue\n            norm.append({\n                \"account\": _to_str(e.get(\"account\")) or \"\",\n                \"debit\": _to_float(e.get(\"debit\")) or 0.0,\n                \"credit\": _to_float(e.get(\"credit\")) or 0.0,\n                \"memo\": _to_str(e.get(\"memo\")),\n            })\n        return {\"entries\": norm, \"confidence\": _to_float(obj.get(\"confidence\")), \"flags\": _to_flags(obj.get(\"flags\"))}\n    return obj\n\ndef vendor_sim(a, b):\n    if not a or not b:\n        return None\n    return fuzz.token_set_ratio(str(a), str(b)) / 100.0\n\ndef amount_close(g, p, rel_tol=0.02, abs_tol=2000.0):\n    if g is None or p is None:\n        return None\n    try:\n        g = float(g); p = float(p)\n        return abs(g - p) <= max(abs_tol, rel_tol * max(abs(g), 1.0))\n    except Exception:\n        return None\n\ndef gold_gate_receipt(gold: dict, pred: dict):\n    if not isinstance(gold, dict) or pred is None:\n        return True\n    ok = True\n    vs = vendor_sim(gold.get(\"vendor_name\"), pred.get(\"vendor_name\"))\n    if vs is not None:\n        ok = ok and (vs >= 0.85)\n    ac = amount_close(gold.get(\"total_amount\"), pred.get(\"total_amount\"))\n    if ac is not None:\n        ok = ok and ac\n    gd, pd = gold.get(\"date\"), pred.get(\"date\")\n    if gd is not None and pd is not None:\n        ok = ok and (str(gd).strip() == str(pd).strip())\n    return ok\n\ndef load_teacher_outputs(path):\n    out = {}\n    if not path:\n        return out\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            r = json.loads(line)\n            out[r[\"id\"]] = r\n    return out\n\nteacher_outputs = {k: load_teacher_outputs(v) for k, v in teacher_paths.items() if v}\n\ndef router_priority(task):\n    if task in [\"receipt_extract_text\", \"invoice_extract_text\"]:\n        return [\"open_finance_8b\", \"finance_llama3_8b\", \"fingpt_lora_llama3_8b\"]\n    if task == \"journal_from_structured_txn\":\n        return [\"finance_llama3_8b\", \"open_finance_8b\", \"fingpt_lora_llama3_8b\"]\n    return [\"finance_llama3_8b\", \"open_finance_8b\", \"fingpt_lora_llama3_8b\"]\n\ndistilled, dropped, picked = [], 0, {}\n\nwith open(kd_pool_path, \"r\", encoding=\"utf-8\") as f:\n    pool = [json.loads(x) for x in f]\n\nfor ex in pool:\n    task, cid, gold = ex[\"task\"], ex[\"id\"], ex.get(\"gold\")\n    chosen_obj, chosen_teacher = None, None\n\n    for tname in router_priority(task):\n        rec = teacher_outputs.get(tname, {}).get(cid)\n        if not rec:\n            continue\n        raw = rec.get(\"raw\", \"\")\n        obj = extract_json_robust(raw)\n        obj = coerce_strict(task, obj) if obj is not None else None\n        if obj is None:\n            continue\n        if not schema_pass(task, obj):\n            continue\n        if task == \"receipt_extract_text\" and isinstance(gold, dict):\n            if not gold_gate_receipt(gold, obj):\n                continue\n        chosen_obj, chosen_teacher = obj, tname\n        break\n\n    if chosen_obj is None:\n        dropped += 1\n        continue\n\n    picked[chosen_teacher] = picked.get(chosen_teacher, 0) + 1\n    distilled.append({\"id\": cid, \"task\": task, \"prompt\": build_prompt(task, ex[\"input\"]), \"answer_json\": chosen_obj})\n\nprint(\"KD distilled:\", len(distilled), \"| dropped:\", dropped)\nprint(\"picked_by_teacher:\", picked)\n\ndistill_path = str(DATA_DIR / \"distilled_train.jsonl\")\nwith open(distill_path, \"w\", encoding=\"utf-8\") as f:\n    for r in distilled:\n        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n\nprint(\"Saved:\", distill_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T09:19:12.131352Z","iopub.execute_input":"2026-01-22T09:19:12.131503Z","iopub.status.idle":"2026-01-22T09:19:18.563642Z","shell.execute_reply.started":"2026-01-22T09:19:12.131487Z","shell.execute_reply":"2026-01-22T09:19:18.563143Z"}},"outputs":[{"name":"stdout","text":"KD distilled: 1198 | dropped: 802\npicked_by_teacher: {'finance_llama3_8b': 1180, 'open_finance_8b': 18}\nSaved: /dev/shm/kaggle_ram/working/data/distilled_train.jsonl\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ============================================================\n# CELL TRAIN (REPLACE) — BF16 Qwen3-32B + LoRA SFT (optim chuẩn)\n# Vị trí: sau distill\n# ============================================================\nfrom datasets import Dataset\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom pathlib import Path\nimport json\n\nassert QWEN32B_PATH is not None, \"Cannot find Qwen3-32B in /kaggle/input\"\n\nqwen_tok, qwen_base = load_bf16_model(QWEN32B_PATH)\n\ndef guess_lora_targets(model):\n    names = set()\n    for n, _ in model.named_modules():\n        if any(k in n for k in [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]):\n            names.add(n.split(\".\")[-1])\n    return sorted(list(names)) if names else [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n\ntargets = guess_lora_targets(qwen_base)\nprint(\"LoRA targets:\", targets)\n\nlora_cfg = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=targets,\n)\n\nstudent = get_peft_model(qwen_base, lora_cfg)\nstudent.print_trainable_parameters()\n\nrows = []\nwith open(distill_path, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        r = json.loads(line)\n        rows.append({\"text\": r[\"prompt\"] + \"\\n\\n\" + json.dumps(r[\"answer_json\"], ensure_ascii=False)})\n\nif len(rows) == 0:\n    raise RuntimeError(\"distilled_train.jsonl is empty (distill produced 0)\")\n\ntrain_ds = Dataset.from_list(rows)\n\nargs = TrainingArguments(\n    output_dir=str(WORKDIR / \"student_ckpt\"),\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    learning_rate=1e-4,\n    num_train_epochs=1,      # tăng lên 2-3 CHỈ KHI EVAL-GOLD tăng\n    warmup_steps=50,\n    logging_steps=10,\n    save_steps=200,\n    bf16=True,\n    optim=\"adamw_torch\",\n    report_to=\"none\",\n)\n\ntrainer = SFTTrainer(\n    model=student,\n    args=args,\n    train_dataset=train_ds,\n    processing_class=qwen_tok,\n)\n\ntrainer.train()\n\nADAPTER_DIR = Path(WORKDIR) / \"outputs\" / \"adapters\" / \"student_adapter\"\nADAPTER_DIR.mkdir(parents=True, exist_ok=True)\ntrainer.model.save_pretrained(str(ADAPTER_DIR))\nqwen_tok.save_pretrained(str(ADAPTER_DIR))\n\nassert (ADAPTER_DIR / \"adapter_config.json\").exists(), f\"Missing adapter_config.json in {ADAPTER_DIR}\"\nprint(\"✅ Saved student adapter:\", str(ADAPTER_DIR))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T09:19:18.564800Z","iopub.execute_input":"2026-01-22T09:19:18.564958Z","iopub.status.idle":"2026-01-22T09:39:28.811447Z","shell.execute_reply.started":"2026-01-22T09:19:18.564942Z","shell.execute_reply":"2026-01-22T09:39:28.810984Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"CUDA device: NVIDIA H100 80GB HBM3\nAttention implementation: sdpa\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc62061a8a4449febc90c60796753682"}},"metadata":{}},{"name":"stdout","text":"LoRA targets: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\ntrainable params: 134,217,728 || all params: 32,896,340,992 || trainable%: 0.4080\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/1198 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b284766b83c9437c89f7d8709342696a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/1198 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ed70f0117324460a5cb05938b47236c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/1198 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f99193732974b9a8610e842ea6f45ed"}},"metadata":{}},{"name":"stderr","text":"The model is already on multiple devices. Skipping the move to device specified in `args`.\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 06:27, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.525800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.354000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.651800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.208100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.156300</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.138200</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.128100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.124100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.116500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.119600</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.116100</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.115500</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.108300</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.107300</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.105700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"✅ Saved student adapter: /dev/shm/kaggle_ram/working/outputs/adapters/student_adapter\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================================\n# CELL EVAL (B-STRICT) (REPLACE) — STRICT, score gold receipts, NO default fill\n# Vị trí: sau TRAIN\n# ============================================================\nimport json, re\nfrom collections import defaultdict\nfrom rapidfuzz import fuzz\nfrom peft import PeftModel\n\ndef _strip_code_fences(s: str) -> str:\n    if not s:\n        return s\n    s = s.strip()\n    s = re.sub(r\"^```(?:json)?\\s*\", \"\", s, flags=re.I)\n    s = re.sub(r\"\\s*```$\", \"\", s)\n    return s.strip()\n\ndef _try_json_load(s: str):\n    try:\n        return json.loads(s)\n    except Exception:\n        return None\n\ndef extract_json_robust(text: str):\n    if text is None:\n        return None\n    t = _strip_code_fences(text)\n    obj = _try_json_load(t)\n    if obj is not None:\n        return obj\n    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", text, flags=re.S | re.I)\n    if m:\n        obj = _try_json_load(m.group(1))\n        if obj is not None:\n            return obj\n    for m in re.finditer(r\"\\{.*?\\}\", t, flags=re.S):\n        obj = _try_json_load(m.group(0))\n        if obj is not None:\n            return obj\n    return None\n\ndef _to_float(x):\n    if x is None:\n        return None\n    if isinstance(x, (int, float)):\n        return float(x)\n    s = str(x).replace(\",\", \".\")\n    s = re.sub(r\"[^0-9\\.\\-]\", \"\", s)\n    try:\n        return float(s) if s else None\n    except Exception:\n        return None\n\ndef _to_str(x):\n    if x is None:\n        return None\n    s = str(x).strip()\n    return s if s else None\n\ndef _to_flags(x):\n    if x is None:\n        return None\n    if isinstance(x, list):\n        return [str(v).strip() for v in x if str(v).strip()]\n    return [p.strip() for p in re.split(r\"[,;\\n]+\", str(x)) if p.strip()]\n\ndef coerce_strict(task: str, obj: dict):\n    if not isinstance(obj, dict):\n        return None\n    if task == \"receipt_extract_text\":\n        return {\n            \"vendor_name\": _to_str(obj.get(\"vendor_name\") or obj.get(\"vendor\") or obj.get(\"seller\")),\n            \"address\": _to_str(obj.get(\"address\") or obj.get(\"addr\")),\n            \"date\": _to_str(obj.get(\"date\")),\n            \"total_amount\": _to_float(obj.get(\"total_amount\") or obj.get(\"total\") or obj.get(\"amount\")),\n            \"currency\": _to_str(obj.get(\"currency\")),\n            \"confidence\": _to_float(obj.get(\"confidence\")),\n            \"flags\": _to_flags(obj.get(\"flags\")),\n        }\n    if task == \"invoice_extract_text\":\n        return {\n            \"vendor_name\": _to_str(obj.get(\"vendor_name\") or obj.get(\"vendor\") or obj.get(\"seller\")),\n            \"invoice_no\": _to_str(obj.get(\"invoice_no\") or obj.get(\"invoice_number\") or obj.get(\"invoice\")),\n            \"date\": _to_str(obj.get(\"date\")),\n            \"subtotal\": _to_float(obj.get(\"subtotal\")),\n            \"tax\": _to_float(obj.get(\"tax\") or obj.get(\"vat\")),\n            \"total\": _to_float(obj.get(\"total\") or obj.get(\"total_amount\") or obj.get(\"amount\")),\n            \"currency\": _to_str(obj.get(\"currency\")),\n            \"confidence\": _to_float(obj.get(\"confidence\")),\n            \"flags\": _to_flags(obj.get(\"flags\")),\n        }\n    if task == \"journal_from_structured_txn\":\n        entries = obj.get(\"entries\")\n        if not isinstance(entries, list):\n            entries = []\n        norm = []\n        for e in entries:\n            if not isinstance(e, dict):\n                continue\n            norm.append({\n                \"account\": _to_str(e.get(\"account\")) or \"\",\n                \"debit\": _to_float(e.get(\"debit\")) or 0.0,\n                \"credit\": _to_float(e.get(\"credit\")) or 0.0,\n                \"memo\": _to_str(e.get(\"memo\")),\n            })\n        return {\"entries\": norm, \"confidence\": _to_float(obj.get(\"confidence\")), \"flags\": _to_flags(obj.get(\"flags\"))}\n    return obj\n\ndef vendor_sim(a, b):\n    if not a or not b:\n        return None\n    return fuzz.token_set_ratio(str(a), str(b)) / 100.0\n\ndef amount_close(g, p, rel_tol=0.02, abs_tol=2000.0):\n    if g is None or p is None:\n        return None\n    try:\n        g = float(g); p = float(p)\n        return abs(g - p) <= max(abs_tol, rel_tol * max(abs(g), 1.0))\n    except Exception:\n        return None\n\ndef eval_b_strict(tok, mdl, cases, batch_size=32, max_new_tokens=256):\n    by_task = defaultdict(list)\n    for c in cases:\n        by_task[c[\"task\"]].append(c)\n\n    per_task = {}\n    for task, exs in by_task.items():\n        n = len(exs)\n        json_valid = 0\n        schema_ok = 0\n        for i in range(0, n, batch_size):\n            batch = exs[i:i+batch_size]\n            prompts = [build_prompt(task, ex[\"input\"]) for ex in batch]\n            texts = generate_batch(tok, mdl, prompts, max_new_tokens=max_new_tokens)\n            for ex, t in zip(batch, texts):\n                raw = extract_json_robust(t)\n                obj = coerce_strict(task, raw) if raw is not None else None\n                if obj is not None:\n                    json_valid += 1\n                    if schema_pass(task, obj):\n                        schema_ok += 1\n        per_task[task] = {\"n\": n, \"json_valid_rate\": json_valid/max(1,n), \"schema_pass_rate\": schema_ok/max(1,n)}\n\n    gold_receipts = [\n        c for c in cases\n        if c[\"task\"] == \"receipt_extract_text\"\n        and isinstance(c.get(\"gold\"), dict)\n        and any(c[\"gold\"].get(k) not in [None, \"\", \"nan\", \"NaN\"] for k in [\"vendor_name\",\"date\",\"total_amount\",\"address\"])\n    ]\n    receipt_gold = None\n    if gold_receipts:\n        vs_list, tc_list, de_list = [], [], []\n        n = len(gold_receipts)\n        for i in range(0, n, batch_size):\n            batch = gold_receipts[i:i+batch_size]\n            prompts = [build_prompt(\"receipt_extract_text\", ex[\"input\"]) for ex in batch]\n            texts = generate_batch(tok, mdl, prompts, max_new_tokens=max_new_tokens)\n            for ex, t in zip(batch, texts):\n                gold = ex[\"gold\"]\n                raw = extract_json_robust(t)\n                obj = coerce_strict(\"receipt_extract_text\", raw) if raw is not None else None\n                vs = vendor_sim(gold.get(\"vendor_name\"), obj.get(\"vendor_name\") if obj else None)\n                if vs is not None:\n                    vs_list.append(vs)\n                ac = amount_close(gold.get(\"total_amount\"), obj.get(\"total_amount\") if obj else None)\n                if ac is not None:\n                    tc_list.append(1.0 if ac else 0.0)\n                gd, pd = gold.get(\"date\"), (obj.get(\"date\") if obj else None)\n                if gd is not None and pd is not None:\n                    de_list.append(1.0 if str(gd).strip() == str(pd).strip() else 0.0)\n\n        def avg(xs): return float(sum(xs)/len(xs)) if xs else None\n        receipt_gold = {\"n\": n, \"vendor_sim_avg\": avg(vs_list), \"total_close_rate\": avg(tc_list), \"date_exact_rate\": avg(de_list)}\n\n    return {\"per_task\": per_task, \"receipt_gold\": receipt_gold}\n\nwith open(eval_path, \"r\", encoding=\"utf-8\") as f:\n    eval_cases = [json.loads(x) for x in f]\n\n# BASE\nbase_tok, base_mdl = load_bf16_model(QWEN32B_PATH)\nbase_rep = eval_b_strict(base_tok, base_mdl, eval_cases, batch_size=32, max_new_tokens=256)\nprint(\"BASE per_task schema:\", {k: v[\"schema_pass_rate\"] for k, v in base_rep[\"per_task\"].items()})\nprint(\"BASE receipt_gold:\", base_rep[\"receipt_gold\"])\n\n# STUDENT\nstudent_tok, student_base = load_bf16_model(QWEN32B_PATH)\nstudent_mdl = PeftModel.from_pretrained(student_base, str(ADAPTER_DIR))\nstudent_mdl.eval()\n\nstudent_rep = eval_b_strict(student_tok, student_mdl, eval_cases, batch_size=32, max_new_tokens=256)\nprint(\"STUDENT per_task schema:\", {k: v[\"schema_pass_rate\"] for k, v in student_rep[\"per_task\"].items()})\nprint(\"STUDENT receipt_gold:\", student_rep[\"receipt_gold\"])\n\nreport = {\n    \"base\": base_rep,\n    \"student\": student_rep,\n    \"meta\": {\n        \"eval_path\": str(eval_path),\n        \"strict\": True,\n        \"no_default_fill\": True,\n        \"batch_size\": 8,\n        \"max_new_tokens\": 256,\n        \"note\": \"If receipt_gold is None => MCOCR has no usable gold columns; must fix dataset/loader first.\",\n    },\n}\n\nreport_path = str(DATA_DIR / \"eval_report_b_strict.json\")\nwith open(report_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(report, f, ensure_ascii=False, indent=2)\n\nprint(\"Saved report:\", report_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T09:39:28.812175Z","iopub.execute_input":"2026-01-22T09:39:28.812348Z","iopub.status.idle":"2026-01-22T09:39:39.081153Z","shell.execute_reply.started":"2026-01-22T09:39:28.812333Z","shell.execute_reply":"2026-01-22T09:39:39.080442Z"}},"outputs":[{"name":"stdout","text":"CUDA device: NVIDIA H100 80GB HBM3\nAttention implementation: sdpa\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"468a35f4e04046ceb1d048f5b9aebf63"}},"metadata":{}},{"name":"stderr","text":"WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_106/2700609826.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;31m# BASE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0mbase_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_bf16_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQWEN32B_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m \u001b[0mbase_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_b_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_cases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BASE per_task schema:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"schema_pass_rate\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbase_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"per_task\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BASE receipt_gold:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"receipt_gold\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_106/2700609826.py\u001b[0m in \u001b[0;36meval_b_strict\u001b[0;34m(tok, mdl, cases, batch_size, max_new_tokens)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mprompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbuild_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_json_robust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_106/151529569.py\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(tok, mdl, prompts, max_new_tokens)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0menc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     out = mdl.generate(\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2565\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2566\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2567\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2568\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2785\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2786\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2787\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2788\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 480\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    481\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    411\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/qwen3/modeling_qwen3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 266.00 MiB. GPU 0 has a total capacity of 79.44 GiB of which 260.62 MiB is free. Process 4495 has 79.18 GiB memory in use. Of the allocated memory 77.75 GiB is allocated by PyTorch, and 716.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 266.00 MiB. GPU 0 has a total capacity of 79.44 GiB of which 260.62 MiB is free. Process 4495 has 79.18 GiB memory in use. Of the allocated memory 77.75 GiB is allocated by PyTorch, and 716.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"# ============================================================\n# CELL SAVE (ADD NEW, cuối notebook) — export adapter + tokenizer + lineage + zip\n# Vị trí: cuối notebook\n# ============================================================\nimport json, shutil\nfrom pathlib import Path\n\nREL = Path(\"/kaggle/working\") / \"release\" / \"qwen3-32b-accounting-distilled-v0.1.0\"\nREL.mkdir(parents=True, exist_ok=True)\n\nADAPTER_OUT = REL / \"adapters\"\nTOKEN_OUT  = REL / \"tokenizer\"\nADAPTER_OUT.mkdir(parents=True, exist_ok=True)\nTOKEN_OUT.mkdir(parents=True, exist_ok=True)\n\ntrainer.model.save_pretrained(str(ADAPTER_OUT))\nqwen_tok.save_pretrained(str(TOKEN_OUT))\n\nlineage = {\n    \"model_name\": \"qwen3-32b-accounting-distilled\",\n    \"version\": \"v0.1.0\",\n    \"method\": \"KD(clean filter) + LoRA SFT(BF16) tuned by GOLD-metric\",\n    \"teachers\": TEACHERS,\n    \"paths\": {\n        \"distill_path\": str(distill_path),\n        \"adapter_dir\": str(ADAPTER_OUT),\n        \"tokenizer_dir\": str(TOKEN_OUT),\n        \"eval_report\": str(DATA_DIR / \"eval_report_b_strict.json\"),\n    },\n}\nwith open(REL / \"lineage.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(lineage, f, ensure_ascii=False, indent=2)\n\nzip_path = shutil.make_archive(str(REL), \"zip\", root_dir=str(REL))\nprint(\"✅ RELEASE folder:\", REL)\nprint(\"✅ ZIP:\", zip_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T09:39:39.081624Z","iopub.status.idle":"2026-01-22T09:39:39.081785Z","shell.execute_reply.started":"2026-01-22T09:39:39.081702Z","shell.execute_reply":"2026-01-22T09:39:39.081715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# CELL SAVE (ADD NEW, cuối notebook) — export adapter + tokenizer + lineage + zip\n# Vị trí: cuối notebook\n# ============================================================\nimport json, shutil\nfrom pathlib import Path\n\nREL = Path(\"/kaggle/working\") / \"release\" / \"qwen3-32b-accounting-distilled-v0.1.0\"\nREL.mkdir(parents=True, exist_ok=True)\n\nADAPTER_OUT = REL / \"adapters\"\nTOKEN_OUT  = REL / \"tokenizer\"\nADAPTER_OUT.mkdir(parents=True, exist_ok=True)\nTOKEN_OUT.mkdir(parents=True, exist_ok=True)\n\ntrainer.model.save_pretrained(str(ADAPTER_OUT))\nqwen_tok.save_pretrained(str(TOKEN_OUT))\n\nlineage = {\n    \"model_name\": \"qwen3-32b-accounting-distilled\",\n    \"version\": \"v0.1.0\",\n    \"method\": \"KD(clean filter) + LoRA SFT(BF16) tuned by GOLD-metric\",\n    \"teachers\": TEACHERS,\n    \"paths\": {\n        \"distill_path\": str(distill_path),\n        \"adapter_dir\": str(ADAPTER_OUT),\n        \"tokenizer_dir\": str(TOKEN_OUT),\n        \"eval_report\": str(DATA_DIR / \"eval_report_b_strict.json\"),\n    },\n}\nwith open(REL / \"lineage.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(lineage, f, ensure_ascii=False, indent=2)\n\nzip_path = shutil.make_archive(str(REL), \"zip\", root_dir=str(REL))\nprint(\"✅ RELEASE folder:\", REL)\nprint(\"✅ ZIP:\", zip_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-22T09:39:39.082506Z","iopub.status.idle":"2026-01-22T09:39:39.082674Z","shell.execute_reply.started":"2026-01-22T09:39:39.082591Z","shell.execute_reply":"2026-01-22T09:39:39.082604Z"}},"outputs":[],"execution_count":null}]}